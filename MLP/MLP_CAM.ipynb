{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062f56a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Softmax\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import vstack\n",
    "from numpy import argmax\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a608b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True  Use << CUDA >>\n",
      "PyTorch Version: 1.7.1+cu110\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('CUDA:', torch.cuda.is_available(), ' Use << {} >>'.format(device.upper()))\n",
    "print('PyTorch Version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4dcf490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, file_name):\n",
    "\n",
    "        dataframe = pd.read_csv(file_name)\n",
    "        print(\"Data looks like\")\n",
    "        x = dataframe.iloc[:,0:9].values\n",
    "        y = dataframe.iloc[:, 9].values # y is label\n",
    "        z = dataframe.iloc[:, 10:15].values # z is about the details and not used for the training\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        return self.x[idx], self.y[idx], self.z[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125432ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, file_name):\n",
    "\n",
    "        dataframe = pd.read_csv(file_name)\n",
    "        \n",
    "        x = dataframe.iloc[:,0:9].values \n",
    "        y = dataframe.iloc[:, 9].values\n",
    "        y = (y-100)/100\n",
    "        z = dataframe.iloc[:, 10:15].values\n",
    "        \n",
    "        #converting to torch tensors\n",
    "        self.x = torch.tensor(x, dtype=torch.float32) \n",
    "        self.y = torch.tensor(y, dtype=torch.int32)\n",
    "        self.z = torch.tensor(z, dtype=torch.int32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        return self.x[idx], self.y[idx], self.z[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6dc9bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data looks like\n",
      "\n",
      "Total Data Length: 18261\n",
      "X_train shape: (12782, 9)\n",
      "X_val shape: (2739, 9)\n",
      "X_test shape: (2740, 9)\n",
      "y_train shape: (12782,)\n",
      "y_val shape: (2739,)\n",
      "y_test shape: (2740,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\widen\\\\Desktop\\\\predict_CAM\\\\MLP\\\\final\")\n",
    "input_data = CSVDataset('raw_base_labelled_input.csv')\n",
    "\n",
    "\n",
    "print(\"\\nTotal Data Length:\", len(input_data))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, z_train, z_test = train_test_split(input_data.x, input_data.y, input_data.z, test_size=0.3, shuffle = True, random_state=42)\n",
    "X_val, X_test, y_val, y_test, z_val, z_test = train_test_split(X_test, y_test, z_test, test_size=0.5, shuffle = True, random_state = 3)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "torch.set_printoptions(precision=10)\n",
    "\n",
    "# Save train data set\n",
    "f = open('2_raw_base_labelled_train_set.csv','w', newline='')\n",
    "wr = csv.writer(f)\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    wr.writerow([X_train[i][0]/100-5.5, X_train[i][1], X_train[i][2], X_train[i][3], X_train[i][4], X_train[i][5], X_train[i][6], X_train[i][7], X_train[i][8], y_train[i] , z_train[i][0], z_train[i][1], z_train[i][2], z_train[i][3], z_train[i][4]])\n",
    "    \n",
    "f.close()\n",
    "\n",
    "# Save validation data set\n",
    "f = open('2_raw_base_labelled_val_set.csv','w', newline='')\n",
    "wr = csv.writer(f)\n",
    "\n",
    "for i in range(len(y_val)):\n",
    "    wr.writerow([X_val[i][0]/100-5.5, X_val[i][1], X_val[i][2], X_val[i][3], X_val[i][4], X_val[i][5], X_val[i][6], X_val[i][7], X_val[i][8], y_val[i], z_val[i][0], z_val[i][1], z_val[i][2], z_val[i][3], z_val[i][4]])\n",
    "    \n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "# Save test data set\n",
    "f = open('2_raw_base_labelled_test_set.csv','w', newline='')\n",
    "wr = csv.writer(f)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    wr.writerow([X_test[i][0]/100-5.5, X_test[i][1], X_test[i][2], X_test[i][3], X_test[i][4], X_test[i][5], X_test[i][6], X_test[i][7], X_test[i][8], y_test[i], z_test[i][0], z_test[i][1], z_test[i][2], z_test[i][3], z_test[i][4]])\n",
    "    \n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f52dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_input):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.act = ReLU()\n",
    "        \n",
    "        self.hidden1 = Linear(n_input, 100).to(device)\n",
    "        \n",
    "        self.hidden2 = Linear(100, 300).to(device)\n",
    "        \n",
    "        self.hidden3 = Linear(300, 100).to(device)\n",
    "        \n",
    "        self.hiddenfinal = Linear(100, 10).to(device)\n",
    "        \n",
    "        #self.actfinal = nn.Softmax(dim=0) \n",
    "        #self.actfinal = nn.LogSoftmax(dim=0) \n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.hidden1(X)\n",
    "        X = self.act(X)\n",
    "        \n",
    "        X = self.hidden2(X)\n",
    "        X = self.act(X)\n",
    "        \n",
    "        X = self.hidden3(X)\n",
    "        X = self.act(X)\n",
    "        \n",
    "        X = self.hiddenfinal(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9602aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = 300\n",
    "batch_size = 40\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d253dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\widen\\\\Desktop\\\\predict_CAM\\\\MLP\\\\final\")\n",
    "trainset = TrainDataset('2_raw_base_labelled_train_set.csv')\n",
    "valset = TrainDataset('2_raw_base_labelled_val_set.csv')\n",
    "testset = TrainDataset('2_raw_base_labelled_test_set.csv')\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(trainset, batch_size, shuffle = True)\n",
    "valLoader = torch.utils.data.DataLoader(valset, batch_size, shuffle = True)\n",
    "testLoader = torch.utils.data.DataLoader(testset, batch_size, shuffle = True)\n",
    "\n",
    "model = MLP(9)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[], 'test_loss':[], 'test_acc':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "896c0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_check_for_batch(labels, preds, batch_size):\n",
    "    total_acc = 0\n",
    "    for i in range(batch_size):        \n",
    "        total_acc += accuracy_check(labels[i], preds[i])\n",
    "    return total_acc/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dff125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_train(model, trainloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch, (inputs, labels, details) in enumerate(trainLoader):\n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device = device, dtype = torch.int64)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1) #\n",
    "            acc = accuracy_check_for_batch(labels, preds, inputs.size()[0])\n",
    "            total_acc += acc\n",
    "            total_loss += loss.cpu().item()\n",
    "        \n",
    "    return total_acc/(batch+1), total_loss/(batch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccbc5dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_check(label, pred):\n",
    "    ims = [label, pred]\n",
    "    np_ims = []\n",
    "    for item in ims:\n",
    "        item = item.cpu().numpy()\n",
    "        np_ims.append(item)\n",
    "    compare = np.equal(np_ims[0], np_ims[1])\n",
    "    accuracy = np.sum(compare)\n",
    "    return accuracy / len(np_ims[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12695207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainLoader, criterion, optimizer, scheduler, device): #scheduler\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, labels, details) in enumerate(trainLoader): # for one batch\n",
    "        current_loss = 0.0\n",
    "        \n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device=device, dtype=torch.int64)\n",
    "        \n",
    "        \n",
    "        criterion = criterion.cuda()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5db713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(model, valLoader, criterion, device):\n",
    "    total_val_loss=0\n",
    "    total_val_acc=0\n",
    "    for batch, (inputs, labels, details) in enumerate(valLoader): # for one batch\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device=device, dtype=torch.int64)\n",
    "        \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "        \n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                acc = accuracy_check_for_batch(labels, preds, inputs.size()[0])\n",
    "                total_val_acc += acc\n",
    "                total_val_loss += loss.cpu().item()\n",
    "                \n",
    "    return total_val_acc/(batch+1), total_val_loss/(batch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "691af376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred(model, finalLoader, device):\n",
    "    f = open('final_2_raw_base_labelled_test_set_predict_results.csv','w', newline='')\n",
    "    for i, (inputs, labels, details) in enumerate(finalLoader):\n",
    "        with torch.no_grad():\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device=device, dtype=torch.int64)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                wr = csv.writer(f)\n",
    "                wr.writerow([inputs[0][0], inputs[0][1], inputs[0][2], inputs[0][3], int(inputs[0][4]), int(inputs[0][5]), int(inputs[0][6]), int(inputs[0][7]), int(inputs[0][8]),  int(labels[0]), int(details[0][0]), int(details[0][1]), int(details[0][2]), int(details[0][3]), int(details[0][4]) ,int(preds[0])])\n",
    "            \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64877b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_pred(model, finalLoader, device):\n",
    "    f = open('final_2_raw_base_labelled_all_set_predict_results.csv','w', newline='')\n",
    "    for i, (inputs, labels, details) in enumerate(finalLoader):\n",
    "        with torch.no_grad():\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device=device, dtype=torch.int64)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                wr = csv.writer(f)\n",
    "                wr.writerow([inputs[0][0], inputs[0][1], inputs[0][2], inputs[0][3], int(inputs[0][4]), int(inputs[0][5]), int(inputs[0][6]), int(inputs[0][7]), int(inputs[0][8]),  int(labels[0]), int(details[0][0]), int(details[0][1]), int(details[0][2]), int(details[0][3]), int(details[0][4]) ,int(preds[0])])\n",
    "            \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38708c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "\n",
      "epoch : 1     train loss : 0.24724         train acc : 0.93212   \n",
      "epoch : 1     val loss : 0.25107           val acc : 0.93144   \n",
      "\n",
      "epoch : 2     train loss : 0.21558         train acc : 0.93291   \n",
      "epoch : 2     val loss : 0.21952           val acc : 0.93325   \n",
      "\n",
      "epoch : 3     train loss : 0.20810         train acc : 0.93281   \n",
      "epoch : 3     val loss : 0.20735           val acc : 0.93333   \n",
      "\n",
      "epoch : 4     train loss : 0.23879         train acc : 0.93384   \n",
      "epoch : 4     val loss : 0.24585           val acc : 0.93217   \n",
      "\n",
      "epoch : 5     train loss : 0.19662         train acc : 0.93040   \n",
      "epoch : 5     val loss : 0.20055           val acc : 0.92911   \n",
      "\n",
      "epoch : 6     train loss : 0.18631         train acc : 0.93594   \n",
      "epoch : 6     val loss : 0.19519           val acc : 0.93535   \n",
      "\n",
      "epoch : 7     train loss : 0.18876         train acc : 0.93781   \n",
      "epoch : 7     val loss : 0.18905           val acc : 0.93986   \n",
      "\n",
      "epoch : 8     train loss : 0.18365         train acc : 0.93587   \n",
      "epoch : 8     val loss : 0.18752           val acc : 0.93716   \n",
      "\n",
      "epoch : 9     train loss : 0.18586         train acc : 0.93570   \n",
      "epoch : 9     val loss : 0.18950           val acc : 0.93506   \n",
      "\n",
      "epoch : 10    train loss : 0.17952         train acc : 0.93612   \n",
      "epoch : 10    val loss : 0.18249           val acc : 0.93949   \n",
      "\n",
      "epoch : 11    train loss : 0.17278         train acc : 0.93900   \n",
      "epoch : 11    val loss : 0.18103           val acc : 0.94014   \n",
      "\n",
      "epoch : 12    train loss : 0.17651         train acc : 0.93712   \n",
      "epoch : 12    val loss : 0.18830           val acc : 0.93470   \n",
      "\n",
      "epoch : 13    train loss : 0.17283         train acc : 0.93947   \n",
      "epoch : 13    val loss : 0.18103           val acc : 0.93688   \n",
      "\n",
      "epoch : 14    train loss : 0.17760         train acc : 0.93797   \n",
      "epoch : 14    val loss : 0.19112           val acc : 0.93824   \n",
      "\n",
      "epoch : 15    train loss : 0.17559         train acc : 0.93727   \n",
      "epoch : 15    val loss : 0.18399           val acc : 0.93833   \n",
      "\n",
      "epoch : 16    train loss : 0.16790         train acc : 0.93956   \n",
      "epoch : 16    val loss : 0.17761           val acc : 0.94114   \n",
      "\n",
      "epoch : 17    train loss : 0.17043         train acc : 0.93798   \n",
      "epoch : 17    val loss : 0.17952           val acc : 0.94022   \n",
      "\n",
      "epoch : 18    train loss : 0.16730         train acc : 0.93962   \n",
      "epoch : 18    val loss : 0.17513           val acc : 0.94094   \n",
      "\n",
      "epoch : 19    train loss : 0.17317         train acc : 0.93830   \n",
      "epoch : 19    val loss : 0.18170           val acc : 0.93869   \n",
      "\n",
      "epoch : 20    train loss : 0.17095         train acc : 0.93939   \n",
      "epoch : 20    val loss : 0.18184           val acc : 0.93688   \n",
      "\n",
      "epoch : 21    train loss : 0.16778         train acc : 0.93822   \n",
      "epoch : 21    val loss : 0.17674           val acc : 0.94022   \n",
      "\n",
      "epoch : 22    train loss : 0.16559         train acc : 0.94017   \n",
      "epoch : 22    val loss : 0.18133           val acc : 0.94106   \n",
      "\n",
      "epoch : 23    train loss : 0.16868         train acc : 0.94166   \n",
      "epoch : 23    val loss : 0.18073           val acc : 0.94412   \n",
      "\n",
      "epoch : 24    train loss : 0.15911         train acc : 0.94189   \n",
      "epoch : 24    val loss : 0.17191           val acc : 0.94050   \n",
      "\n",
      "epoch : 25    train loss : 0.16022         train acc : 0.94219   \n",
      "epoch : 25    val loss : 0.17814           val acc : 0.94086   \n",
      "\n",
      "epoch : 26    train loss : 0.15813         train acc : 0.94283   \n",
      "epoch : 26    val loss : 0.17449           val acc : 0.94203   \n",
      "\n",
      "epoch : 27    train loss : 0.15673         train acc : 0.94290   \n",
      "epoch : 27    val loss : 0.17902           val acc : 0.94304   \n",
      "\n",
      "epoch : 28    train loss : 0.16364         train acc : 0.94173   \n",
      "epoch : 28    val loss : 0.18175           val acc : 0.93768   \n",
      "\n",
      "epoch : 29    train loss : 0.15802         train acc : 0.94219   \n",
      "epoch : 29    val loss : 0.17721           val acc : 0.94094   \n",
      "\n",
      "epoch : 30    train loss : 0.15614         train acc : 0.94242   \n",
      "epoch : 30    val loss : 0.17618           val acc : 0.94493   \n",
      "\n",
      "epoch : 31    train loss : 0.15369         train acc : 0.94422   \n",
      "epoch : 31    val loss : 0.17215           val acc : 0.94195   \n",
      "\n",
      "epoch : 32    train loss : 0.15155         train acc : 0.94353   \n",
      "epoch : 32    val loss : 0.17068           val acc : 0.94384   \n",
      "\n",
      "epoch : 33    train loss : 0.15486         train acc : 0.94282   \n",
      "epoch : 33    val loss : 0.17760           val acc : 0.94340   \n",
      "\n",
      "epoch : 34    train loss : 0.15438         train acc : 0.94259   \n",
      "epoch : 34    val loss : 0.17361           val acc : 0.94022   \n",
      "\n",
      "epoch : 35    train loss : 0.15340         train acc : 0.94360   \n",
      "epoch : 35    val loss : 0.17784           val acc : 0.93913   \n",
      "\n",
      "epoch : 36    train loss : 0.15004         train acc : 0.94580   \n",
      "epoch : 36    val loss : 0.17438           val acc : 0.94231   \n",
      "\n",
      "epoch : 37    train loss : 0.16027         train acc : 0.93992   \n",
      "epoch : 37    val loss : 0.18332           val acc : 0.93615   \n",
      "\n",
      "epoch : 38    train loss : 0.14896         train acc : 0.94307   \n",
      "epoch : 38    val loss : 0.17245           val acc : 0.94050   \n",
      "\n",
      "epoch : 39    train loss : 0.14643         train acc : 0.94633   \n",
      "epoch : 39    val loss : 0.17216           val acc : 0.94384   \n",
      "\n",
      "epoch : 40    train loss : 0.14727         train acc : 0.94570   \n",
      "epoch : 40    val loss : 0.17595           val acc : 0.94295   \n",
      "\n",
      "epoch : 41    train loss : 0.14949         train acc : 0.94634   \n",
      "epoch : 41    val loss : 0.17723           val acc : 0.94420   \n",
      "\n",
      "epoch : 42    train loss : 0.14899         train acc : 0.94812   \n",
      "epoch : 42    val loss : 0.18002           val acc : 0.94457   \n",
      "\n",
      "epoch : 43    train loss : 0.14287         train acc : 0.94555   \n",
      "epoch : 43    val loss : 0.17312           val acc : 0.94384   \n",
      "\n",
      "epoch : 44    train loss : 0.15070         train acc : 0.94376   \n",
      "epoch : 44    val loss : 0.18242           val acc : 0.93716   \n",
      "\n",
      "epoch : 45    train loss : 0.14351         train acc : 0.94797   \n",
      "epoch : 45    val loss : 0.17678           val acc : 0.94324   \n",
      "\n",
      "epoch : 46    train loss : 0.14441         train acc : 0.94695   \n",
      "epoch : 46    val loss : 0.18021           val acc : 0.94058   \n",
      "\n",
      "epoch : 47    train loss : 0.14529         train acc : 0.94798   \n",
      "epoch : 47    val loss : 0.17828           val acc : 0.94420   \n",
      "\n",
      "epoch : 48    train loss : 0.14610         train acc : 0.94493   \n",
      "epoch : 48    val loss : 0.17702           val acc : 0.94203   \n",
      "\n",
      "epoch : 49    train loss : 0.14615         train acc : 0.94650   \n",
      "epoch : 49    val loss : 0.17969           val acc : 0.94058   \n",
      "\n",
      "epoch : 50    train loss : 0.14138         train acc : 0.94797   \n",
      "epoch : 50    val loss : 0.17876           val acc : 0.94215   \n",
      "\n",
      "epoch : 51    train loss : 0.13851         train acc : 0.94868   \n",
      "epoch : 51    val loss : 0.17384           val acc : 0.94565   \n",
      "\n",
      "epoch : 52    train loss : 0.13766         train acc : 0.95039   \n",
      "epoch : 52    val loss : 0.17723           val acc : 0.94593   \n",
      "\n",
      "epoch : 53    train loss : 0.13832         train acc : 0.94984   \n",
      "epoch : 53    val loss : 0.17607           val acc : 0.94457   \n",
      "\n",
      "epoch : 54    train loss : 0.14181         train acc : 0.94791   \n",
      "epoch : 54    val loss : 0.17940           val acc : 0.94159   \n",
      "\n",
      "epoch : 55    train loss : 0.14118         train acc : 0.94844   \n",
      "epoch : 55    val loss : 0.18392           val acc : 0.93905   \n",
      "\n",
      "epoch : 56    train loss : 0.14166         train acc : 0.94767   \n",
      "epoch : 56    val loss : 0.18252           val acc : 0.93861   \n",
      "\n",
      "epoch : 57    train loss : 0.13713         train acc : 0.95103   \n",
      "epoch : 57    val loss : 0.17707           val acc : 0.94376   \n",
      "\n",
      "epoch : 58    train loss : 0.13427         train acc : 0.94969   \n",
      "epoch : 58    val loss : 0.17762           val acc : 0.94457   \n",
      "\n",
      "epoch : 59    train loss : 0.13675         train acc : 0.94946   \n",
      "epoch : 59    val loss : 0.18327           val acc : 0.94006   \n",
      "\n",
      "epoch : 60    train loss : 0.13563         train acc : 0.94962   \n",
      "epoch : 60    val loss : 0.18270           val acc : 0.94295   \n",
      "\n",
      "epoch : 61    train loss : 0.14156         train acc : 0.94945   \n",
      "epoch : 61    val loss : 0.18500           val acc : 0.94231   \n",
      "\n",
      "epoch : 62    train loss : 0.13248         train acc : 0.95102   \n",
      "epoch : 62    val loss : 0.18539           val acc : 0.94070   \n",
      "\n",
      "epoch : 63    train loss : 0.13248         train acc : 0.95126   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 63    val loss : 0.17990           val acc : 0.94086   \n",
      "\n",
      "epoch : 64    train loss : 0.13161         train acc : 0.95242   \n",
      "epoch : 64    val loss : 0.18011           val acc : 0.94259   \n",
      "\n",
      "epoch : 65    train loss : 0.13940         train acc : 0.95134   \n",
      "epoch : 65    val loss : 0.18557           val acc : 0.94746   \n",
      "\n",
      "epoch : 66    train loss : 0.13128         train acc : 0.95102   \n",
      "epoch : 66    val loss : 0.18027           val acc : 0.94239   \n",
      "\n",
      "epoch : 67    train loss : 0.13275         train acc : 0.95406   \n",
      "epoch : 67    val loss : 0.18159           val acc : 0.94340   \n",
      "\n",
      "epoch : 68    train loss : 0.12992         train acc : 0.95281   \n",
      "epoch : 68    val loss : 0.18116           val acc : 0.94203   \n",
      "\n",
      "epoch : 69    train loss : 0.13300         train acc : 0.95102   \n",
      "epoch : 69    val loss : 0.18501           val acc : 0.94159   \n",
      "\n",
      "epoch : 70    train loss : 0.12944         train acc : 0.95298   \n",
      "epoch : 70    val loss : 0.18005           val acc : 0.94529   \n",
      "\n",
      "epoch : 71    train loss : 0.12982         train acc : 0.95336   \n",
      "epoch : 71    val loss : 0.18545           val acc : 0.93986   \n",
      "\n",
      "epoch : 72    train loss : 0.13008         train acc : 0.95367   \n",
      "epoch : 72    val loss : 0.18549           val acc : 0.94122   \n",
      "\n",
      "epoch : 73    train loss : 0.12947         train acc : 0.95367   \n",
      "epoch : 73    val loss : 0.18368           val acc : 0.94304   \n",
      "\n",
      "epoch : 74    train loss : 0.12829         train acc : 0.95438   \n",
      "epoch : 74    val loss : 0.18471           val acc : 0.94324   \n",
      "\n",
      "epoch : 75    train loss : 0.12738         train acc : 0.95359   \n",
      "epoch : 75    val loss : 0.18450           val acc : 0.94477   \n",
      "\n",
      "epoch : 76    train loss : 0.12773         train acc : 0.95446   \n",
      "epoch : 76    val loss : 0.18078           val acc : 0.94050   \n",
      "\n",
      "epoch : 77    train loss : 0.12617         train acc : 0.95346   \n",
      "epoch : 77    val loss : 0.18376           val acc : 0.94167   \n",
      "\n",
      "epoch : 78    train loss : 0.12639         train acc : 0.95453   \n",
      "epoch : 78    val loss : 0.18335           val acc : 0.94239   \n",
      "\n",
      "epoch : 79    train loss : 0.12781         train acc : 0.95376   \n",
      "epoch : 79    val loss : 0.18480           val acc : 0.94050   \n",
      "\n",
      "epoch : 80    train loss : 0.12559         train acc : 0.95477   \n",
      "epoch : 80    val loss : 0.18319           val acc : 0.94267   \n",
      "\n",
      "epoch : 81    train loss : 0.12480         train acc : 0.95494   \n",
      "epoch : 81    val loss : 0.18535           val acc : 0.94348   \n",
      "\n",
      "epoch : 82    train loss : 0.12579         train acc : 0.95540   \n",
      "epoch : 82    val loss : 0.18798           val acc : 0.94187   \n",
      "\n",
      "epoch : 83    train loss : 0.12724         train acc : 0.95406   \n",
      "epoch : 83    val loss : 0.18475           val acc : 0.94239   \n",
      "\n",
      "epoch : 84    train loss : 0.12608         train acc : 0.95407   \n",
      "epoch : 84    val loss : 0.18444           val acc : 0.94006   \n",
      "\n",
      "epoch : 85    train loss : 0.12382         train acc : 0.95563   \n",
      "epoch : 85    val loss : 0.18684           val acc : 0.94493   \n",
      "\n",
      "epoch : 86    train loss : 0.12341         train acc : 0.95602   \n",
      "epoch : 86    val loss : 0.18681           val acc : 0.94231   \n",
      "\n",
      "epoch : 87    train loss : 0.12561         train acc : 0.95502   \n",
      "epoch : 87    val loss : 0.18634           val acc : 0.94167   \n",
      "\n",
      "epoch : 88    train loss : 0.12421         train acc : 0.95548   \n",
      "epoch : 88    val loss : 0.19149           val acc : 0.94122   \n",
      "\n",
      "epoch : 89    train loss : 0.12397         train acc : 0.95470   \n",
      "epoch : 89    val loss : 0.19011           val acc : 0.94050   \n",
      "\n",
      "epoch : 90    train loss : 0.12438         train acc : 0.95602   \n",
      "epoch : 90    val loss : 0.19036           val acc : 0.94340   \n",
      "\n",
      "epoch : 91    train loss : 0.12406         train acc : 0.95594   \n",
      "epoch : 91    val loss : 0.18860           val acc : 0.93941   \n",
      "\n",
      "epoch : 92    train loss : 0.12290         train acc : 0.95540   \n",
      "epoch : 92    val loss : 0.19087           val acc : 0.93969   \n",
      "\n",
      "epoch : 93    train loss : 0.12221         train acc : 0.95727   \n",
      "epoch : 93    val loss : 0.18623           val acc : 0.94239   \n",
      "\n",
      "epoch : 94    train loss : 0.12564         train acc : 0.95523   \n",
      "epoch : 94    val loss : 0.19367           val acc : 0.93897   \n",
      "\n",
      "epoch : 95    train loss : 0.12203         train acc : 0.95688   \n",
      "epoch : 95    val loss : 0.19239           val acc : 0.94094   \n",
      "\n",
      "epoch : 96    train loss : 0.12302         train acc : 0.95588   \n",
      "epoch : 96    val loss : 0.18888           val acc : 0.94094   \n",
      "\n",
      "Stop Training!\n",
      "\n",
      "epoch : 97    train loss : 0.12191         train acc : 0.95572   \n",
      "epoch : 97    val loss : 0.18846           val acc : 0.94267   \n",
      "\n",
      "epoch : 98    train loss : 0.12217         train acc : 0.95617   \n",
      "epoch : 98    val loss : 0.19263           val acc : 0.93869   \n",
      "\n",
      "epoch : 99    train loss : 0.12285         train acc : 0.95555   \n",
      "epoch : 99    val loss : 0.19471           val acc : 0.94042   \n",
      "\n",
      "epoch : 100   train loss : 0.12158         train acc : 0.95555   \n",
      "epoch : 100   val loss : 0.19451           val acc : 0.93845   \n",
      "\n",
      "epoch : 101   train loss : 0.12066         train acc : 0.95688   \n",
      "epoch : 101   val loss : 0.19505           val acc : 0.93853   \n",
      "\n",
      "epoch : 102   train loss : 0.12034         train acc : 0.95735   \n",
      "epoch : 102   val loss : 0.18923           val acc : 0.94078   \n",
      "\n",
      "epoch : 103   train loss : 0.12027         train acc : 0.95711   \n",
      "epoch : 103   val loss : 0.19164           val acc : 0.94078   \n",
      "\n",
      "epoch : 104   train loss : 0.12154         train acc : 0.95634   \n",
      "epoch : 104   val loss : 0.19538           val acc : 0.93933   \n",
      "\n",
      "epoch : 105   train loss : 0.11875         train acc : 0.95618   \n",
      "epoch : 105   val loss : 0.18918           val acc : 0.94239   \n",
      "\n",
      "epoch : 106   train loss : 0.11881         train acc : 0.95727   \n",
      "epoch : 106   val loss : 0.18996           val acc : 0.94159   \n",
      "\n",
      "epoch : 107   train loss : 0.11998         train acc : 0.95648   \n",
      "epoch : 107   val loss : 0.19490           val acc : 0.93977   \n",
      "\n",
      "epoch : 108   train loss : 0.11896         train acc : 0.95611   \n",
      "epoch : 108   val loss : 0.19309           val acc : 0.93861   \n",
      "\n",
      "epoch : 109   train loss : 0.11928         train acc : 0.95641   \n",
      "epoch : 109   val loss : 0.19415           val acc : 0.93905   \n",
      "\n",
      "epoch : 110   train loss : 0.11919         train acc : 0.95618   \n",
      "epoch : 110   val loss : 0.19561           val acc : 0.93925   \n",
      "\n",
      "epoch : 111   train loss : 0.11870         train acc : 0.95798   \n",
      "epoch : 111   val loss : 0.19379           val acc : 0.94122   \n",
      "\n",
      "epoch : 112   train loss : 0.12228         train acc : 0.95531   \n",
      "epoch : 112   val loss : 0.19478           val acc : 0.93986   \n",
      "\n",
      "epoch : 113   train loss : 0.11910         train acc : 0.95728   \n",
      "epoch : 113   val loss : 0.19345           val acc : 0.94050   \n",
      "\n",
      "epoch : 114   train loss : 0.11840         train acc : 0.95673   \n",
      "epoch : 114   val loss : 0.19690           val acc : 0.93953   \n",
      "\n",
      "epoch : 115   train loss : 0.11798         train acc : 0.95797   \n",
      "epoch : 115   val loss : 0.19284           val acc : 0.93986   \n",
      "\n",
      "epoch : 116   train loss : 0.11720         train acc : 0.95751   \n",
      "epoch : 116   val loss : 0.19625           val acc : 0.93897   \n",
      "\n",
      "epoch : 117   train loss : 0.11716         train acc : 0.95673   \n",
      "epoch : 117   val loss : 0.19728           val acc : 0.93953   \n",
      "\n",
      "epoch : 118   train loss : 0.11687         train acc : 0.95789   \n",
      "epoch : 118   val loss : 0.19799           val acc : 0.94034   \n",
      "\n",
      "epoch : 119   train loss : 0.11764         train acc : 0.95751   \n",
      "epoch : 119   val loss : 0.19422           val acc : 0.93925   \n",
      "\n",
      "epoch : 120   train loss : 0.11731         train acc : 0.95837   \n",
      "epoch : 120   val loss : 0.19947           val acc : 0.93897   \n",
      "\n",
      "epoch : 121   train loss : 0.11762         train acc : 0.95673   \n",
      "epoch : 121   val loss : 0.19859           val acc : 0.94014   \n",
      "\n",
      "epoch : 122   train loss : 0.11700         train acc : 0.95791   \n",
      "epoch : 122   val loss : 0.19642           val acc : 0.94130   \n",
      "\n",
      "epoch : 123   train loss : 0.11657         train acc : 0.95695   \n",
      "epoch : 123   val loss : 0.19518           val acc : 0.94130   \n",
      "\n",
      "epoch : 124   train loss : 0.11744         train acc : 0.95687   \n",
      "epoch : 124   val loss : 0.19538           val acc : 0.94195   \n",
      "\n",
      "epoch : 125   train loss : 0.11679         train acc : 0.95781   \n",
      "epoch : 125   val loss : 0.19677           val acc : 0.94114   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 126   train loss : 0.11670         train acc : 0.95884   \n",
      "epoch : 126   val loss : 0.19572           val acc : 0.94122   \n",
      "\n",
      "epoch : 127   train loss : 0.11756         train acc : 0.95703   \n",
      "epoch : 127   val loss : 0.19751           val acc : 0.94151   \n",
      "\n",
      "epoch : 128   train loss : 0.11623         train acc : 0.95797   \n",
      "epoch : 128   val loss : 0.19787           val acc : 0.94130   \n",
      "\n",
      "epoch : 129   train loss : 0.11659         train acc : 0.95687   \n",
      "epoch : 129   val loss : 0.19865           val acc : 0.93941   \n",
      "\n",
      "epoch : 130   train loss : 0.11600         train acc : 0.95759   \n",
      "epoch : 130   val loss : 0.19604           val acc : 0.94203   \n",
      "\n",
      "epoch : 131   train loss : 0.11621         train acc : 0.95712   \n",
      "epoch : 131   val loss : 0.19652           val acc : 0.94058   \n",
      "\n",
      "epoch : 132   train loss : 0.11647         train acc : 0.95853   \n",
      "epoch : 132   val loss : 0.19864           val acc : 0.93788   \n",
      "\n",
      "epoch : 133   train loss : 0.11550         train acc : 0.95695   \n",
      "epoch : 133   val loss : 0.19632           val acc : 0.94078   \n",
      "\n",
      "epoch : 134   train loss : 0.11563         train acc : 0.95782   \n",
      "epoch : 134   val loss : 0.20334           val acc : 0.93845   \n",
      "\n",
      "epoch : 135   train loss : 0.11577         train acc : 0.95829   \n",
      "epoch : 135   val loss : 0.19665           val acc : 0.94106   \n",
      "\n",
      "epoch : 136   train loss : 0.11526         train acc : 0.95805   \n",
      "epoch : 136   val loss : 0.19685           val acc : 0.94167   \n",
      "\n",
      "epoch : 137   train loss : 0.11576         train acc : 0.95735   \n",
      "epoch : 137   val loss : 0.19694           val acc : 0.93977   \n",
      "\n",
      "epoch : 138   train loss : 0.11578         train acc : 0.95789   \n",
      "epoch : 138   val loss : 0.19776           val acc : 0.94050   \n",
      "\n",
      "epoch : 139   train loss : 0.11587         train acc : 0.95790   \n",
      "epoch : 139   val loss : 0.19690           val acc : 0.93905   \n",
      "\n",
      "epoch : 140   train loss : 0.11532         train acc : 0.95836   \n",
      "epoch : 140   val loss : 0.19691           val acc : 0.94086   \n",
      "\n",
      "epoch : 141   train loss : 0.11518         train acc : 0.95829   \n",
      "epoch : 141   val loss : 0.19931           val acc : 0.94086   \n",
      "\n",
      "epoch : 142   train loss : 0.11486         train acc : 0.95868   \n",
      "epoch : 142   val loss : 0.19678           val acc : 0.94058   \n",
      "\n",
      "epoch : 143   train loss : 0.11529         train acc : 0.95737   \n",
      "epoch : 143   val loss : 0.19935           val acc : 0.93977   \n",
      "\n",
      "epoch : 144   train loss : 0.11465         train acc : 0.95867   \n",
      "epoch : 144   val loss : 0.19696           val acc : 0.94058   \n",
      "\n",
      "epoch : 145   train loss : 0.11459         train acc : 0.95844   \n",
      "epoch : 145   val loss : 0.19811           val acc : 0.94094   \n",
      "\n",
      "epoch : 146   train loss : 0.11544         train acc : 0.95759   \n",
      "epoch : 146   val loss : 0.19775           val acc : 0.94086   \n",
      "\n",
      "epoch : 147   train loss : 0.11523         train acc : 0.95805   \n",
      "epoch : 147   val loss : 0.20292           val acc : 0.93780   \n",
      "\n",
      "epoch : 148   train loss : 0.11513         train acc : 0.95891   \n",
      "epoch : 148   val loss : 0.20027           val acc : 0.94014   \n",
      "\n",
      "epoch : 149   train loss : 0.11473         train acc : 0.95783   \n",
      "epoch : 149   val loss : 0.19799           val acc : 0.94086   \n",
      "\n",
      "epoch : 150   train loss : 0.11462         train acc : 0.95844   \n",
      "epoch : 150   val loss : 0.19851           val acc : 0.94014   \n",
      "\n",
      "epoch : 151   train loss : 0.11458         train acc : 0.95798   \n",
      "epoch : 151   val loss : 0.19760           val acc : 0.94094   \n",
      "\n",
      "epoch : 152   train loss : 0.11413         train acc : 0.95852   \n",
      "epoch : 152   val loss : 0.19849           val acc : 0.94167   \n",
      "\n",
      "epoch : 153   train loss : 0.11454         train acc : 0.95789   \n",
      "epoch : 153   val loss : 0.20008           val acc : 0.94086   \n",
      "\n",
      "epoch : 154   train loss : 0.11456         train acc : 0.95821   \n",
      "epoch : 154   val loss : 0.19860           val acc : 0.94122   \n",
      "\n",
      "epoch : 155   train loss : 0.11506         train acc : 0.95805   \n",
      "epoch : 155   val loss : 0.20148           val acc : 0.93941   \n",
      "\n",
      "epoch : 156   train loss : 0.11392         train acc : 0.95875   \n",
      "epoch : 156   val loss : 0.19941           val acc : 0.94006   \n",
      "\n",
      "epoch : 157   train loss : 0.11442         train acc : 0.95852   \n",
      "epoch : 157   val loss : 0.19889           val acc : 0.94086   \n",
      "\n",
      "epoch : 158   train loss : 0.11418         train acc : 0.95806   \n",
      "epoch : 158   val loss : 0.19947           val acc : 0.94006   \n",
      "\n",
      "epoch : 159   train loss : 0.11392         train acc : 0.95852   \n",
      "epoch : 159   val loss : 0.19976           val acc : 0.94058   \n",
      "\n",
      "epoch : 160   train loss : 0.11391         train acc : 0.95876   \n",
      "epoch : 160   val loss : 0.20001           val acc : 0.94058   \n",
      "\n",
      "epoch : 161   train loss : 0.11379         train acc : 0.95883   \n",
      "epoch : 161   val loss : 0.19949           val acc : 0.93949   \n",
      "\n",
      "epoch : 162   train loss : 0.11395         train acc : 0.95869   \n",
      "epoch : 162   val loss : 0.19900           val acc : 0.94167   \n",
      "\n",
      "epoch : 163   train loss : 0.11407         train acc : 0.95829   \n",
      "epoch : 163   val loss : 0.20102           val acc : 0.94042   \n",
      "\n",
      "epoch : 164   train loss : 0.11417         train acc : 0.95852   \n",
      "epoch : 164   val loss : 0.19925           val acc : 0.94094   \n",
      "\n",
      "epoch : 165   train loss : 0.11379         train acc : 0.95891   \n",
      "epoch : 165   val loss : 0.19953           val acc : 0.94094   \n",
      "\n",
      "epoch : 166   train loss : 0.11385         train acc : 0.95899   \n",
      "epoch : 166   val loss : 0.20237           val acc : 0.94014   \n",
      "\n",
      "epoch : 167   train loss : 0.11382         train acc : 0.95821   \n",
      "epoch : 167   val loss : 0.19948           val acc : 0.94050   \n",
      "\n",
      "epoch : 168   train loss : 0.11393         train acc : 0.95852   \n",
      "epoch : 168   val loss : 0.19975           val acc : 0.94094   \n",
      "\n",
      "epoch : 169   train loss : 0.11363         train acc : 0.95867   \n",
      "epoch : 169   val loss : 0.20025           val acc : 0.94130   \n",
      "\n",
      "epoch : 170   train loss : 0.11368         train acc : 0.95876   \n",
      "epoch : 170   val loss : 0.20141           val acc : 0.94050   \n",
      "\n",
      "epoch : 171   train loss : 0.11395         train acc : 0.95829   \n",
      "epoch : 171   val loss : 0.20042           val acc : 0.94014   \n",
      "\n",
      "epoch : 172   train loss : 0.11339         train acc : 0.95867   \n",
      "epoch : 172   val loss : 0.20211           val acc : 0.94006   \n",
      "\n",
      "epoch : 173   train loss : 0.11345         train acc : 0.95907   \n",
      "epoch : 173   val loss : 0.20177           val acc : 0.93861   \n",
      "\n",
      "epoch : 174   train loss : 0.11338         train acc : 0.95860   \n",
      "epoch : 174   val loss : 0.20064           val acc : 0.94094   \n",
      "\n",
      "epoch : 175   train loss : 0.11393         train acc : 0.95931   \n",
      "epoch : 175   val loss : 0.20022           val acc : 0.94094   \n",
      "\n",
      "epoch : 176   train loss : 0.11379         train acc : 0.95876   \n",
      "epoch : 176   val loss : 0.20051           val acc : 0.94014   \n",
      "\n",
      "epoch : 177   train loss : 0.11326         train acc : 0.95852   \n",
      "epoch : 177   val loss : 0.20372           val acc : 0.93961   \n",
      "\n",
      "epoch : 178   train loss : 0.11310         train acc : 0.95891   \n",
      "epoch : 178   val loss : 0.20236           val acc : 0.93969   \n",
      "\n",
      "epoch : 179   train loss : 0.11353         train acc : 0.95884   \n",
      "epoch : 179   val loss : 0.20122           val acc : 0.94130   \n",
      "\n",
      "epoch : 180   train loss : 0.11314         train acc : 0.95899   \n",
      "epoch : 180   val loss : 0.20121           val acc : 0.94014   \n",
      "\n",
      "epoch : 181   train loss : 0.11305         train acc : 0.95906   \n",
      "epoch : 181   val loss : 0.20020           val acc : 0.94058   \n",
      "\n",
      "epoch : 182   train loss : 0.11309         train acc : 0.95852   \n",
      "epoch : 182   val loss : 0.20061           val acc : 0.94050   \n",
      "\n",
      "epoch : 183   train loss : 0.11311         train acc : 0.95876   \n",
      "epoch : 183   val loss : 0.20082           val acc : 0.94094   \n",
      "\n",
      "epoch : 184   train loss : 0.11317         train acc : 0.95853   \n",
      "epoch : 184   val loss : 0.20109           val acc : 0.94058   \n",
      "\n",
      "epoch : 185   train loss : 0.11292         train acc : 0.95914   \n",
      "epoch : 185   val loss : 0.20227           val acc : 0.94050   \n",
      "\n",
      "epoch : 186   train loss : 0.11312         train acc : 0.95891   \n",
      "epoch : 186   val loss : 0.20198           val acc : 0.94014   \n",
      "\n",
      "epoch : 187   train loss : 0.11320         train acc : 0.95875   \n",
      "epoch : 187   val loss : 0.20312           val acc : 0.94014   \n",
      "\n",
      "epoch : 188   train loss : 0.11300         train acc : 0.95907   \n",
      "epoch : 188   val loss : 0.20184           val acc : 0.94050   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 189   train loss : 0.11300         train acc : 0.95899   \n",
      "epoch : 189   val loss : 0.20140           val acc : 0.94058   \n",
      "\n",
      "epoch : 190   train loss : 0.11280         train acc : 0.95922   \n",
      "epoch : 190   val loss : 0.20159           val acc : 0.94014   \n",
      "\n",
      "epoch : 191   train loss : 0.11304         train acc : 0.95900   \n",
      "epoch : 191   val loss : 0.20161           val acc : 0.94094   \n",
      "\n",
      "epoch : 192   train loss : 0.11274         train acc : 0.95922   \n",
      "epoch : 192   val loss : 0.20137           val acc : 0.94058   \n",
      "\n",
      "epoch : 193   train loss : 0.11283         train acc : 0.95906   \n",
      "epoch : 193   val loss : 0.20187           val acc : 0.94094   \n",
      "\n",
      "epoch : 194   train loss : 0.11306         train acc : 0.95876   \n",
      "epoch : 194   val loss : 0.20258           val acc : 0.93977   \n",
      "\n",
      "epoch : 195   train loss : 0.11313         train acc : 0.95893   \n",
      "epoch : 195   val loss : 0.20410           val acc : 0.94014   \n",
      "\n",
      "epoch : 196   train loss : 0.11282         train acc : 0.95922   \n",
      "epoch : 196   val loss : 0.20402           val acc : 0.94014   \n",
      "\n",
      "epoch : 197   train loss : 0.11274         train acc : 0.95930   \n",
      "epoch : 197   val loss : 0.20344           val acc : 0.93969   \n",
      "\n",
      "epoch : 198   train loss : 0.11293         train acc : 0.95884   \n",
      "epoch : 198   val loss : 0.20279           val acc : 0.93933   \n",
      "\n",
      "epoch : 199   train loss : 0.11267         train acc : 0.95922   \n",
      "epoch : 199   val loss : 0.20245           val acc : 0.94050   \n",
      "\n",
      "epoch : 200   train loss : 0.11311         train acc : 0.95869   \n",
      "epoch : 200   val loss : 0.20396           val acc : 0.94050   \n",
      "\n",
      "epoch : 201   train loss : 0.11261         train acc : 0.95906   \n",
      "epoch : 201   val loss : 0.20455           val acc : 0.94014   \n",
      "\n",
      "epoch : 202   train loss : 0.11288         train acc : 0.95916   \n",
      "epoch : 202   val loss : 0.20186           val acc : 0.94022   \n",
      "\n",
      "epoch : 203   train loss : 0.11276         train acc : 0.95954   \n",
      "epoch : 203   val loss : 0.20202           val acc : 0.93986   \n",
      "\n",
      "epoch : 204   train loss : 0.11259         train acc : 0.95937   \n",
      "epoch : 204   val loss : 0.20213           val acc : 0.94022   \n",
      "\n",
      "epoch : 205   train loss : 0.11256         train acc : 0.95937   \n",
      "epoch : 205   val loss : 0.20418           val acc : 0.93969   \n",
      "\n",
      "epoch : 206   train loss : 0.11262         train acc : 0.95930   \n",
      "epoch : 206   val loss : 0.20181           val acc : 0.93986   \n",
      "\n",
      "epoch : 207   train loss : 0.11277         train acc : 0.95954   \n",
      "epoch : 207   val loss : 0.20376           val acc : 0.93897   \n",
      "\n",
      "epoch : 208   train loss : 0.11261         train acc : 0.95922   \n",
      "epoch : 208   val loss : 0.20488           val acc : 0.94050   \n",
      "\n",
      "epoch : 209   train loss : 0.11249         train acc : 0.95914   \n",
      "epoch : 209   val loss : 0.20322           val acc : 0.94050   \n",
      "\n",
      "epoch : 210   train loss : 0.11258         train acc : 0.95906   \n",
      "epoch : 210   val loss : 0.20177           val acc : 0.94058   \n",
      "\n",
      "epoch : 211   train loss : 0.11266         train acc : 0.95923   \n",
      "epoch : 211   val loss : 0.20231           val acc : 0.93977   \n",
      "\n",
      "epoch : 212   train loss : 0.11251         train acc : 0.95930   \n",
      "epoch : 212   val loss : 0.20316           val acc : 0.94014   \n",
      "\n",
      "epoch : 213   train loss : 0.11270         train acc : 0.95931   \n",
      "epoch : 213   val loss : 0.20245           val acc : 0.94014   \n",
      "\n",
      "epoch : 214   train loss : 0.11252         train acc : 0.95923   \n",
      "epoch : 214   val loss : 0.20992           val acc : 0.93881   \n",
      "\n",
      "epoch : 215   train loss : 0.11240         train acc : 0.95945   \n",
      "epoch : 215   val loss : 0.21168           val acc : 0.93969   \n",
      "\n",
      "epoch : 216   train loss : 0.11243         train acc : 0.95937   \n",
      "epoch : 216   val loss : 0.20349           val acc : 0.94050   \n",
      "\n",
      "epoch : 217   train loss : 0.11245         train acc : 0.95923   \n",
      "epoch : 217   val loss : 0.20532           val acc : 0.93969   \n",
      "\n",
      "epoch : 218   train loss : 0.11248         train acc : 0.95938   \n",
      "epoch : 218   val loss : 0.20294           val acc : 0.93977   \n",
      "\n",
      "epoch : 219   train loss : 0.11251         train acc : 0.95930   \n",
      "epoch : 219   val loss : 0.20271           val acc : 0.94050   \n",
      "\n",
      "epoch : 220   train loss : 0.11242         train acc : 0.95938   \n",
      "epoch : 220   val loss : 0.20386           val acc : 0.94006   \n",
      "\n",
      "epoch : 221   train loss : 0.11239         train acc : 0.95938   \n",
      "epoch : 221   val loss : 0.20376           val acc : 0.93969   \n",
      "\n",
      "epoch : 222   train loss : 0.11233         train acc : 0.95922   \n",
      "epoch : 222   val loss : 0.20462           val acc : 0.93889   \n",
      "\n",
      "epoch : 223   train loss : 0.11235         train acc : 0.95938   \n",
      "epoch : 223   val loss : 0.20233           val acc : 0.94094   \n",
      "\n",
      "epoch : 224   train loss : 0.11239         train acc : 0.95938   \n",
      "epoch : 224   val loss : 0.20243           val acc : 0.94022   \n",
      "\n",
      "epoch : 225   train loss : 0.11236         train acc : 0.95938   \n",
      "epoch : 225   val loss : 0.20259           val acc : 0.94022   \n",
      "\n",
      "epoch : 226   train loss : 0.11237         train acc : 0.95930   \n",
      "epoch : 226   val loss : 0.20250           val acc : 0.94094   \n",
      "\n",
      "epoch : 227   train loss : 0.11223         train acc : 0.95945   \n",
      "epoch : 227   val loss : 0.20275           val acc : 0.94058   \n",
      "\n",
      "epoch : 228   train loss : 0.11236         train acc : 0.95923   \n",
      "epoch : 228   val loss : 0.20218           val acc : 0.94058   \n",
      "\n",
      "epoch : 229   train loss : 0.11228         train acc : 0.95937   \n",
      "epoch : 229   val loss : 0.20290           val acc : 0.93986   \n",
      "\n",
      "epoch : 230   train loss : 0.11228         train acc : 0.95937   \n",
      "epoch : 230   val loss : 0.20293           val acc : 0.93941   \n",
      "\n",
      "epoch : 231   train loss : 0.11220         train acc : 0.95930   \n",
      "epoch : 231   val loss : 0.20352           val acc : 0.93977   \n",
      "\n",
      "epoch : 232   train loss : 0.11232         train acc : 0.95945   \n",
      "epoch : 232   val loss : 0.20267           val acc : 0.93986   \n",
      "\n",
      "epoch : 233   train loss : 0.11228         train acc : 0.95923   \n",
      "epoch : 233   val loss : 0.20493           val acc : 0.93889   \n",
      "\n",
      "epoch : 234   train loss : 0.11227         train acc : 0.95938   \n",
      "epoch : 234   val loss : 0.20643           val acc : 0.93969   \n",
      "\n",
      "epoch : 235   train loss : 0.11248         train acc : 0.95931   \n",
      "epoch : 235   val loss : 0.20285           val acc : 0.94022   \n",
      "\n",
      "epoch : 236   train loss : 0.11244         train acc : 0.95923   \n",
      "epoch : 236   val loss : 0.20310           val acc : 0.94058   \n",
      "\n",
      "epoch : 237   train loss : 0.11238         train acc : 0.95946   \n",
      "epoch : 237   val loss : 0.20381           val acc : 0.93933   \n",
      "\n",
      "epoch : 238   train loss : 0.11242         train acc : 0.95932   \n",
      "epoch : 238   val loss : 0.20287           val acc : 0.94014   \n",
      "\n",
      "epoch : 239   train loss : 0.11221         train acc : 0.95961   \n",
      "epoch : 239   val loss : 0.20296           val acc : 0.94058   \n",
      "\n",
      "epoch : 240   train loss : 0.11250         train acc : 0.95938   \n",
      "epoch : 240   val loss : 0.20287           val acc : 0.93986   \n",
      "\n",
      "epoch : 241   train loss : 0.11234         train acc : 0.95962   \n",
      "epoch : 241   val loss : 0.20648           val acc : 0.93941   \n",
      "\n",
      "epoch : 242   train loss : 0.11229         train acc : 0.95938   \n",
      "epoch : 242   val loss : 0.20278           val acc : 0.94058   \n",
      "\n",
      "epoch : 243   train loss : 0.11212         train acc : 0.95969   \n",
      "epoch : 243   val loss : 0.20388           val acc : 0.93941   \n",
      "\n",
      "epoch : 244   train loss : 0.11280         train acc : 0.95916   \n",
      "epoch : 244   val loss : 0.20283           val acc : 0.94022   \n",
      "\n",
      "epoch : 245   train loss : 0.11253         train acc : 0.95938   \n",
      "epoch : 245   val loss : 0.20383           val acc : 0.94050   \n",
      "\n",
      "epoch : 246   train loss : 0.11240         train acc : 0.95916   \n",
      "epoch : 246   val loss : 0.20511           val acc : 0.93941   \n",
      "\n",
      "epoch : 247   train loss : 0.11233         train acc : 0.95916   \n",
      "epoch : 247   val loss : 0.20271           val acc : 0.94022   \n",
      "\n",
      "epoch : 248   train loss : 0.11217         train acc : 0.95930   \n",
      "epoch : 248   val loss : 0.20519           val acc : 0.93969   \n",
      "\n",
      "epoch : 249   train loss : 0.11228         train acc : 0.95923   \n",
      "epoch : 249   val loss : 0.20332           val acc : 0.94022   \n",
      "\n",
      "epoch : 250   train loss : 0.11219         train acc : 0.95938   \n",
      "epoch : 250   val loss : 0.20431           val acc : 0.93897   \n",
      "\n",
      "epoch : 251   train loss : 0.11230         train acc : 0.95938   \n",
      "epoch : 251   val loss : 0.20288           val acc : 0.94022   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 252   train loss : 0.11251         train acc : 0.95931   \n",
      "epoch : 252   val loss : 0.20396           val acc : 0.93897   \n",
      "\n",
      "epoch : 253   train loss : 0.11221         train acc : 0.95931   \n",
      "epoch : 253   val loss : 0.20594           val acc : 0.93933   \n",
      "\n",
      "epoch : 254   train loss : 0.11219         train acc : 0.95930   \n",
      "epoch : 254   val loss : 0.20408           val acc : 0.93933   \n",
      "\n",
      "epoch : 255   train loss : 0.11207         train acc : 0.95937   \n",
      "epoch : 255   val loss : 0.20616           val acc : 0.93889   \n",
      "\n",
      "epoch : 256   train loss : 0.11229         train acc : 0.95930   \n",
      "epoch : 256   val loss : 0.20376           val acc : 0.93941   \n",
      "\n",
      "epoch : 257   train loss : 0.11206         train acc : 0.95961   \n",
      "epoch : 257   val loss : 0.20373           val acc : 0.93977   \n",
      "\n",
      "epoch : 258   train loss : 0.11222         train acc : 0.95946   \n",
      "epoch : 258   val loss : 0.20298           val acc : 0.94058   \n",
      "\n",
      "epoch : 259   train loss : 0.11232         train acc : 0.95931   \n",
      "epoch : 259   val loss : 0.20332           val acc : 0.94022   \n",
      "\n",
      "epoch : 260   train loss : 0.11213         train acc : 0.95930   \n",
      "epoch : 260   val loss : 0.20356           val acc : 0.93977   \n",
      "\n",
      "epoch : 261   train loss : 0.11214         train acc : 0.95930   \n",
      "epoch : 261   val loss : 0.20669           val acc : 0.93969   \n",
      "\n",
      "epoch : 262   train loss : 0.11213         train acc : 0.95938   \n",
      "epoch : 262   val loss : 0.20389           val acc : 0.94022   \n",
      "\n",
      "epoch : 263   train loss : 0.11209         train acc : 0.95945   \n",
      "epoch : 263   val loss : 0.20317           val acc : 0.94022   \n",
      "\n",
      "epoch : 264   train loss : 0.11211         train acc : 0.95945   \n",
      "epoch : 264   val loss : 0.20584           val acc : 0.93933   \n",
      "\n",
      "epoch : 265   train loss : 0.11211         train acc : 0.95946   \n",
      "epoch : 265   val loss : 0.20566           val acc : 0.94006   \n",
      "\n",
      "epoch : 266   train loss : 0.11199         train acc : 0.95945   \n",
      "epoch : 266   val loss : 0.20722           val acc : 0.93933   \n",
      "\n",
      "epoch : 267   train loss : 0.11220         train acc : 0.95931   \n",
      "epoch : 267   val loss : 0.20424           val acc : 0.93977   \n",
      "\n",
      "epoch : 268   train loss : 0.11199         train acc : 0.95945   \n",
      "epoch : 268   val loss : 0.20648           val acc : 0.93933   \n",
      "\n",
      "epoch : 269   train loss : 0.11202         train acc : 0.95953   \n",
      "epoch : 269   val loss : 0.20362           val acc : 0.93977   \n",
      "\n",
      "epoch : 270   train loss : 0.11216         train acc : 0.95938   \n",
      "epoch : 270   val loss : 0.20631           val acc : 0.94014   \n",
      "\n",
      "epoch : 271   train loss : 0.11196         train acc : 0.95937   \n",
      "epoch : 271   val loss : 0.20309           val acc : 0.94022   \n",
      "\n",
      "epoch : 272   train loss : 0.11206         train acc : 0.95945   \n",
      "epoch : 272   val loss : 0.20355           val acc : 0.93977   \n",
      "\n",
      "epoch : 273   train loss : 0.11238         train acc : 0.95939   \n",
      "epoch : 273   val loss : 0.20317           val acc : 0.94022   \n",
      "\n",
      "epoch : 274   train loss : 0.11213         train acc : 0.95930   \n",
      "epoch : 274   val loss : 0.20300           val acc : 0.94022   \n",
      "\n",
      "epoch : 275   train loss : 0.11246         train acc : 0.95932   \n",
      "epoch : 275   val loss : 0.20559           val acc : 0.93889   \n",
      "\n",
      "epoch : 276   train loss : 0.11198         train acc : 0.95953   \n",
      "epoch : 276   val loss : 0.20339           val acc : 0.94022   \n",
      "\n",
      "epoch : 277   train loss : 0.11216         train acc : 0.95939   \n",
      "epoch : 277   val loss : 0.20429           val acc : 0.94014   \n",
      "\n",
      "epoch : 278   train loss : 0.11203         train acc : 0.95937   \n",
      "epoch : 278   val loss : 0.20384           val acc : 0.93977   \n",
      "\n",
      "epoch : 279   train loss : 0.11196         train acc : 0.95945   \n",
      "epoch : 279   val loss : 0.20890           val acc : 0.93889   \n",
      "\n",
      "epoch : 280   train loss : 0.11201         train acc : 0.95937   \n",
      "epoch : 280   val loss : 0.20566           val acc : 0.93933   \n",
      "\n",
      "epoch : 281   train loss : 0.11215         train acc : 0.95938   \n",
      "epoch : 281   val loss : 0.20423           val acc : 0.93977   \n",
      "\n",
      "epoch : 282   train loss : 0.11194         train acc : 0.95961   \n",
      "epoch : 282   val loss : 0.20568           val acc : 0.93977   \n",
      "\n",
      "epoch : 283   train loss : 0.11194         train acc : 0.95953   \n",
      "epoch : 283   val loss : 0.20368           val acc : 0.94022   \n",
      "\n",
      "epoch : 284   train loss : 0.11206         train acc : 0.95953   \n",
      "epoch : 284   val loss : 0.20478           val acc : 0.93977   \n",
      "\n",
      "epoch : 285   train loss : 0.11199         train acc : 0.95938   \n",
      "epoch : 285   val loss : 0.20315           val acc : 0.94022   \n",
      "\n",
      "epoch : 286   train loss : 0.11225         train acc : 0.95938   \n",
      "epoch : 286   val loss : 0.20447           val acc : 0.93933   \n",
      "\n",
      "epoch : 287   train loss : 0.11196         train acc : 0.95945   \n",
      "epoch : 287   val loss : 0.20345           val acc : 0.94022   \n",
      "\n",
      "epoch : 288   train loss : 0.11212         train acc : 0.95946   \n",
      "epoch : 288   val loss : 0.20456           val acc : 0.93933   \n",
      "\n",
      "epoch : 289   train loss : 0.11196         train acc : 0.95945   \n",
      "epoch : 289   val loss : 0.20350           val acc : 0.94022   \n",
      "\n",
      "epoch : 290   train loss : 0.11218         train acc : 0.95946   \n",
      "epoch : 290   val loss : 0.20396           val acc : 0.93977   \n",
      "\n",
      "epoch : 291   train loss : 0.11199         train acc : 0.95953   \n",
      "epoch : 291   val loss : 0.20384           val acc : 0.94022   \n",
      "\n",
      "epoch : 292   train loss : 0.11191         train acc : 0.95945   \n",
      "epoch : 292   val loss : 0.20678           val acc : 0.93845   \n",
      "\n",
      "epoch : 293   train loss : 0.11225         train acc : 0.95931   \n",
      "epoch : 293   val loss : 0.20375           val acc : 0.93977   \n",
      "\n",
      "epoch : 294   train loss : 0.11192         train acc : 0.95953   \n",
      "epoch : 294   val loss : 0.20327           val acc : 0.94022   \n",
      "\n",
      "epoch : 295   train loss : 0.11192         train acc : 0.95953   \n",
      "epoch : 295   val loss : 0.20423           val acc : 0.93977   \n",
      "\n",
      "epoch : 296   train loss : 0.11203         train acc : 0.95954   \n",
      "epoch : 296   val loss : 0.20338           val acc : 0.94022   \n",
      "\n",
      "epoch : 297   train loss : 0.11250         train acc : 0.95923   \n",
      "epoch : 297   val loss : 0.20457           val acc : 0.93977   \n",
      "\n",
      "epoch : 298   train loss : 0.11197         train acc : 0.95945   \n",
      "epoch : 298   val loss : 0.20477           val acc : 0.94014   \n",
      "\n",
      "epoch : 299   train loss : 0.11191         train acc : 0.95953   \n",
      "epoch : 299   val loss : 0.20465           val acc : 0.93977   \n",
      "\n",
      "epoch : 300   train loss : 0.11203         train acc : 0.95946   \n",
      "epoch : 300   val loss : 0.20464           val acc : 0.93977   \n",
      "\n",
      "\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\\n\")\n",
    "best_acc = None\n",
    "count = 0\n",
    "flag = 0\n",
    "print_count = 1\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    \n",
    "    train_model(model, trainLoader, criterion, optimizer, scheduler, device)\n",
    "    train_acc, train_loss = get_loss_train(model, trainLoader, criterion, device)\n",
    "    print(\"epoch : {:<5} train loss : {:<15.5f} train acc : {:<10.5f}\".format(epoch+1, train_loss, train_acc))\n",
    "    \n",
    "    val_acc, val_loss = val_model(model, valLoader, criterion, device)\n",
    "    print(\"epoch : {:<5} val loss : {:<17.5f} val acc : {:<10.5f}\\n\".format(epoch+1, val_loss, val_acc))\n",
    "    \n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if best_acc == None:\n",
    "        best_acc = val_acc\n",
    "    elif val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        flag = epoch+1\n",
    "        count = 0\n",
    "    else:\n",
    "        count += 1\n",
    "        \n",
    "        if count > 30 and print_count == 1:\n",
    "            print(\"Stop Training!\\n\")\n",
    "            print_count = 0\n",
    "    \n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    torch.save(model.state_dict(), './fianl_shallow_model'+str(epoch+1)+'.pth')\n",
    "    \n",
    "\n",
    "print(\"\\nTraining Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e48d8bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stop Training at 65 th step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (act): ReLU()\n",
       "  (hidden1): Linear(in_features=9, out_features=100, bias=True)\n",
       "  (hidden2): Linear(in_features=100, out_features=300, bias=True)\n",
       "  (hidden3): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (hiddenfinal): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nStop Training at\",flag,\"th step\")\n",
    "PATH = 'C:\\\\Users\\\\widen\\\\Desktop\\\\predict_CAM\\\\MLP\\\\final\\\\final_shallow_model65.pth'\n",
    "\n",
    "model = MLP(9)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6273b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\widen\\\\Desktop\\\\predict_CAM\\\\MLP\\\\final\")\n",
    "finalset = TrainDataset('2_raw_base_labelled_test_set.csv')\n",
    "finalLoader = torch.utils.data.DataLoader(finalset, batch_size=1, shuffle = False)\n",
    "\n",
    "print_pred(model, finalLoader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eca3b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\widen\\\\Desktop\\\\predict_CAM\\\\MLP\\\\final\")\n",
    "allset = TrainDataset('temp_transformed_all_trace_not_shuffled.csv')\n",
    "allLoader = torch.utils.data.DataLoader(allset, batch_size=1, shuffle = False)\n",
    "\n",
    "print_all_pred(model, allLoader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31727c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAACgCAYAAAAWy/vJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA8GklEQVR4nO2dd3gUVdfAfycJoffQO1KkiQJ2AfnwVUCpLxYUxN5719eCvffeUMSGFcGKINgbEEAC0nsNvZdkz/fHmWE2yW6yQDYJ5P6eZ5+ZuW3O3Nm5595zm6gqDofD4XBkJ6GwBXA4HA5H0cQpCIfD4XBExCkIh8PhcETEKQiHw+FwRMQpCIfD4XBExCkIh8PhcETEKQiHo4ggIieKyNJc/F8RkbsKUiZH8cYpCMcBg4hMEJH1IlKysGUpDFT1MlW9P69wIrJQRE4qCJkcBzdOQTgOCESkIdARUKBXAd87qSDvV5gUp2d15I1TEI4DhXOBP4C3gcHhHiJST0Q+E5F0EVkrIi+E+V0sIjNFZLOIzBCRdp67ikiTsHBvi8gD3vmJIrJURG4VkZXAWyJSWUS+9O6x3juvGxa/ioi8JSLLPf+Rnvt0EekZFq6EiKwRkSOiPaiI3Cgiq0VkhYicH0XGFE+GDSKyTkR+FpEEERkO1AdGi8gWEbnFC99LRNK88BNEpEVYugu9Z50GbBWRm0Xk02wyPSciz+b5lhwHFU5BOA4UzgXe836niEgNABFJBL4EFgENgTrAh57f6cAQL24FrOWxNsb71QSqAA2AS7Bv5S3vuj6wHXghLPxwoAzQCqgOPO25vwMMDAvXA1ihqqm53Lei9xwXAi+KSOUI4W4ElgLVgBrAHYCq6iBgMdBTVcup6mMi0gz4ALjOC/81pkCSw9IbAJwKVALeBbqJSCXY06o4y3sWRzHCKQhHkUdETsAK5o9UdRIwDzjb8z4KqA3crKpbVXWHqv7i+V0EPKaqf6sxV1UXxXjbEHCPqu5U1e2qulZVP1XVbaq6GXgQ6OzJVwvoDlymqutVdbeq/uil8y7QQ0QqeNeDMGUSjd3AfV4aXwNbgOZRwtUCGnhhf9boC6udCXylqt+r6m7gCaA0cFxYmOdUdYn3rCuAn4DTPb9uwBov7x3FCKcgHAcCg4ExqrrGu36fwMxUD1ikqhkR4tXDlMm+kK6qO/wLESkjIq+KyCIR2YQVoJW8Fkw9YJ2qrs+eiKouB34F/uvVyLtjraBorM32LNuAchHCPQ7MBcaIyHwRuS2XNGtjLSxfphCwBGul+CzJFmcYQctnILkrNcdBiuuQchRpRKQ0cAaQ6PUHAJTECue2WMFWX0SSIiiJJcAhUZLehpmEfGpiJhuf7LXxG7Ga/NGqulJEDgdSAfHuU0VEKqnqhgj3Goa1ZpKA31V1WbTnjRWvFXMjcKOItAZ+EJG/VXVcBNmXA238CxERTKmFy5E9zkjgZS/t04Bb9ldmx4GHa0E4ijp9gEygJXC492sB/Iz1LfwFrAAeEZGyIlJKRI734r4B3CQi7cVoIiINPL8pwNkikigi3fDMRblQHut32CAiVYB7fA/PJPMN8JLXmV1CRDqFxR0JtAOuJZ/s+CJymvc8AmzE8ijkea8CGocF/wg4VUS6ikgJTLHsBH6Llr7XevoEa639paqL80Nux4GFUxCOos5g4C1VXayqK/0f1kF8DlaD7wk0wTpnl2I2d1T1Y6yv4H1gM1ZQV/HSvdaLt8FLZ2QecjyD2e3XYKOpvs3mPwjrF/gXWI11COPJsR34FGgEfBbzk+dOU2As1kfxO/CSqo73/B4G7vRGLN2kqrMwM9Hznvw9sU7sXXncYxjW8nDmpWKKuA2DHI74IyJ3A81UdWCegYsIIlIfU3g1VXVTYcvjKHhcH4TDEWc8k9SFWCvjgEBEEoAbgA+dcii+OBOTwxFHRORirBP7G1X9qbDliQURKQtsAv5DWF+Lo/jhTEwOh8PhiIhrQTgcDocjInFVECLSTURmicjcSBN5RKSBiIwTkWne+jDha9vUF5ExYuvozBBbrM3hcDgcBUTcTEzeDNPZmB1zKfA3MEBVZ4SF+Rj4UlWHicj/Aed7a8kgIhOAB1X1exEpB4RUdVu0+6WkpGjDhg3j8iwOh8NxsDJp0qQ1qlotkl88RzEdBcxV1fkAIvIh0BuYERamJTZSAmA83lh0EWkJJKnq9wCquiWvmzVs2JCJEyfmm/AOh8NRHBCRqOuTxdPEVIes67ssJevaLwBTgX7eeV+gvIhUBZphM1Y/E5FUEXnca5FkQUQuEZGJIjIxPT09Do/gcDgcxZfC7qS+CegsIqnYUgfLsCUDkrDNYW4CjsSWDTgve2RVfU1VO6hqh2rVIraQHA6HI3+YOxe25GnMiB+hECxeDGlp4HcNxHkUajwVxDJsQTCfumRdHAxVXa6q/VT1COB/ntsGrLUxRVXnewuwjcTWsnE4HAcju/Ja9SOfWLUKxo6F2bPzDrtjBzz+OEydCqmp0LIldOwIGzYEYbZutUJ62TJ47z24914YNy5nWs88AwMGwM8/W/h16+CBB+Cxx+Cmm6B6dUv7uuvsvEULuPNOWL/eFMNHH0GTJtCgAbRuDddeC488AikpMGJEPmVOBFQ1Lj+sFTAfW38mGTMntcoWJgVI8M4fxNbBB0j0wlfzrt8Crsztfu3bt1eHw7EXhEKqZ56p+vrr8bvH6tWq48bZ+fbtql98obp1a9Yw336rWqGC6o8/xpZmZqbq/ferDh+uunNn4L54seott6h26aJ6+OGqV16pmppqfrt2qd56q6qIKqhWq6aanm5+S5eqvv226g03qD72mGpGhurcuapHHGFhy5VTbdjQ4pQooXr00aq//6564YXmX7q0Hf1fYqLqsGGqjz5qcr7wgrknJdmxUSPV6tWzhu/d29wTEuz8pJPsPCVFtXZtC3fYYaovvaR61VVB3OrVLf4nn+zzKwImarRyPJpHfvyw3bNmY2vy/89zuw/o5Z33B+Z4Yd4ASobF/Q8wDfgH22YyObd7OQXhcIQRCqm++25QCEZi5EgrAtq2zT2tq69WPf/8rG5paao7dkQOv2iR6ssvq771lmqtWnaPyZNVH3jAzlNSVD/4wMJu3656yCHm3rWr6ooVVvCmpqpOmqTasqXqxx9nTf/BB4MCslEj1enTVZ94wgrgpCTVY4+1ArZMGdXkZFMabdpY+AsvVH3/fSvoe/ZU7d49SCs5OZCjYkXVSpVUhw5Vbd3aFMsPP6h++qkpMz/OZZeZYnn0UZM5PV31mGMCf18hdeqkun696ptvqvbooXryyRZ+wwbVNWvsuTIyVDdtCp4zNVX1tNNU+/ZV/fBD8/d5+21TkJs22fN26JDVfy8oNAVRkD+nIBwHLfPnq65aldN91SrVZcuyumVm2vH99+3zvuUWu960yZSGTyhktWy/IFu2THXBgpwKZd06KzgTE60gC4VU777b4tSpYwVeeNhff1WtUSNIt2lT1ZIlrSBt2FC1XTvVI49ULVtWdckS1SFDLFyvXnZs1syO5curVq2qe2rod95pNfjDDzdZzjpL9csvVWvWtPRBtV8/1YULA3nWrFHt1s38WrTIWsu+915zr1DBzqdOtQL2ySfNvUMHyw8/76ZNC+KuX6/6+OOq338f+X2tW6f60EMWZ84cC7t2beSw+UG4ktkHnIJwOA5UQiHV+vVVTz89p3v79laQjh1rbvPmqVaubLVkv+bevLkVHpUqqd5zTxD/s8/M/4Yb7PjMM1YAd+xo/lu3mvnmlVeCwn7oUNW77rLzM85QPe44Ox81SnXQoCBc/fqqf/6pOnGipXPOOWYuAWs5zJtnhXrTpuZ21llWCFeubNcvvmi19lq1TOHUq2fuJ56oevzxqkcdpbpxo8m5YIHV2G+9NVCO4WRmWuGf3W/XLnue5ctzxpk9O6vp6iDHKQiHIz8JhXLW3GNh+nQzFWQvfDIzI7cQVK1w8wtdVauJbt5sNnC/BlyihNnvr7nGatci9jvvPAszeLDuqYkvX26FY/Pmqoceaue1awe1cLA+gRYtzPTTpo2dN2hg54mJpgxCITMPtW0bFP5XXWUKZeXKrM8wfrz5V6kSmKX8lsPZZwf58fXXqiNG2PmuXapbttj5vHnmF94CcuQbTkE4HPnJu++arXv+fLuOVHPNzvjxVtsHqxH//nvgd/PNVkD/80/OeA8/HBTca9ZYbblePTOdlC9vZppDDjFbfLlyVnhPmGCKaOnSIG7btibzueeavRysw1hV9YILdI+JJiXFZBGxVgeoPvJI0NKoXDmrGWrWLKvpDxkS/dlDIWuZ3H9/4JaRofrLL7HlnSOuOAXhcOQnp59un85rr1lhXKZMMFomEosWqZYqZR2uH35otfHGja0lkJ5u8cFMJ9k7Gjt1CjpPP/ggKPDBRumomgy+2+TJWeMfdZTuMQNde20QrmPHoEb+ww8m04IFqvfdZ/533qk6c6b1HaxZo/rXX+b+8ss5n8/V7A9onIJwOPKLjIzAVj5ggOpFF9n5aadFj3P33VYj9zs9f/rJrgcNCgpt37Z/xRVmohkyJBiZc8kl5nfkkXZ8/HGzxc+dG9zjwQdVL700572HD1c99VSrqWdkWH/FK68EsmRnxw7rn9i9O6ffggVOGRyE5KYgDpr9IDp06KBuLSbHPrNrF7z/Ppx2mk0+CufTT23C0lFHwbZtcOyxULkylCxpk5i2brXf779Dq1YweLDFGzECEhKgUSOb+PTdd0Gat90Gjz5q5717w+efww032ISqhARL1+enn2DgQJtFW768TbJKcptBOvIHEZmkqh0iekbTHAfaz7UgHPvM4sWBKeacc7L6LVwYjGUHG3MuYsMYfbfXX7cRQBUqBJOd/NbAxx/bud/56hMK2VyCp57KWpv//HPruJ061cw6r7xiYXv3zrul4nDsAzgTk8PhsXy5zQ0I72jt1886eE85xQr/L75Q7dzZ+gvuucfcfv3VJl/5Y+Rnz7bzEiVsHPqkSTaZrEMHGwV0002BAqlaNfqksljxx+0//fT+peNwZCM3BeHaqY7iw5Qp0LcvLFwIyclw//22+NrXX8PFF8M995g5qHdvCz95MpQrByefDMcdByNHwmWXwamnBuvitGwJFStCu3YwdGhwr5NOgiOPNHNQu3Zmjtof/u//4KGH7N4ORwHh+iAcBy+ZmZCYaLb7nj1h2jSoVg3q1oX0dFMUn3wCZ50FP/4InTrB88/DW2/Bww9Dv37W5/Dxx9C/f8705861PoEaNQrmeXbvhhIlCuZejmJDbn0Qhb3ct8Oxf8ydaytiZmbCb79Bjx6wZo11CNeoAb/+ai2D2bPhuedg+nS44w5YuhTGjDEFUaMGHH+8pXf11dZyOOUUePFFc+/VK/K9mzQpOOUATjk4ChxnYnIUPXbvhltvtSWNGzSIHm7rViu8Z840BfHOO/Dvv7ZE8w8/wNq1cM45pgyuucYKf7A4KSl2vWwZnH++tTSyc9559nM4iilOQTiKHhMnwtNPmwJ49dWc/qmpNiz0r79MIRxxhLUKwNbKf/JJUxiDBsHw4VC6tCkcn+RkGDIEXn8dDj/c+hUcDkcOnIJwFD3S0uz47rtmPqpY0a63bbP5Ay++aOODkpLgwQetlt+mDZxwAjzxBBx6KNSpYwrg6KOhQoWcpqArr7Sfw+GIilMQjqJHWppNFtu2zVoAV10FCxaYaWj6dLu+/36oVCmIM3u2KYKkJBtNVKeOjRxySsDh2GfcKCZH0ePkk214aEKCbfs4bZq1Ej791DqVTzmlsCV0OA4a3CgmR9Hm6adtf16f6dNtyYqzz4Z//rHhqGPH2gglpxwcjgLDKQhH4ZKebh3MQ4aYGWn9elixwjqbu3e3MM88Y6ONunYtTEkdjmKHUxCOwuWVV8yMlJBgw1P9DupWraBZM5vZ/NJL5uYUhMNRoLhOakfhsXmzjUjq3t1mNw8dakNSwRSEiJmVXnzR5kM0bly48jocxYy4tiBEpJuIzBKRuSJyWwT/BiIyTkSmicgEEakb5pcpIlO836h4yumIA3Pnwl132XyESAwbBg0bwqpVcMstNny1dGl46ilb/6h+fQvnm5m6djWF4XA4CoyYFISIfCYip4pIzApFRBKBF4HuQEtggIi0zBbsCeAdVT0MuA94OMxvu6oe7v2irHXgKLIMH24dz2PH5vTbtQuuu85aBL//DieeaOczZtheChdfHCiDLl3s52Y0OxwFTqwF/kvA2cAcEXlERJrHEOcoYK6qzlfVXcCHQO9sYVoCP3jn4yP4O4oqu3fDFVfAhAmR/f/9147hK5z6/PADbNhgayQdc0zgXqsWvP22tSJ8ypSx8B075pPgDocjVmJSEKo6VlXPAdoBC4GxIvKbiJwvItFWEKsDLAm7Xuq5hTMV6Oed9wXKi0hV77qUiEwUkT9EpE8scjoKkB9/hJdftuWnf/89p/+sWXYcOdLWRArnk09sFdT//CfuYjocjn1nb0xGVYHzgIuAVOBZTGF8vx/3vwnoLCKpQGdgGeAbrRt4kzfOBp4RkUMiyHSJp0Qmpqen74cYjr1m9GgoVQpq17Y9FsL7GkIhm9nctauZk+66C+bPty09R40ypdGz5/7vkeBwOOJKTKOYRORzoDkwHOipqis8rxEiEm368jKgXth1Xc9tD6q6HK8FISLlgP+q6gbPb5l3nC8iE4AjgHnZ4r8GvAY2kzqWZ3HkA6qmIE46yVZLHTDAlsg+8kjzX7oUtm+H00+3NZBeftl+4UTaX8HhcBQpYm1BPKeqLVX14TDlAEC0KdrA30BTEWkkIsnAWUCW0UgikhLW8X07MNRzrywiJf0wwPHAjBhldcSbGTNsUlvPnrbTGWTtjPbNS82bw3vvwaRJ1q/w11+2B8PTT1tch8NRpIl1HkRLEUn1a/ciUhkYoKovRYugqhkichXwHZAIDFXVNBG5D9sDdRRwIvCwiCjwE+CvrNYCeFVEQpgSe0RVnYIoKowcacdTT4Xq1eGww0xB3H67ufsd1M29sQzt2tnPx/U9OBwHBDEt1iciU1T18Gxuqap6RLwE21vcYn0FxNy5tv/CscdaawDgxhvhhRdsyYyvv7ad1kaPho0b3dwFh6OIkx+L9SWKBF+6N8chOT+EcxQiY8faDObwUUbff28jlAC++Qa+/DLwy8iw/oYSJeDNNwP3k06yzughQ8yM9P77tieDUw4OxwFNrAriW6xDuquIdAU+8NwcBzLvvmuL4IUPU73+eutcXr7cVlO9+ebA7/vvbbe3556DemHjDzp1MlPTOecEO7s1j2WqjMPhKMrE2gdxK3ApcLl3/T3wRlwkchQMoZC1EMA6kU87zUYnLVxoW3126WKT2TZvhp07bUjqiBG2u9vpp2dNq2xZWLTIhr3u2gXz5sEZZ+yVOEuXQtu2MG6c7QLqcDgKn5gUhKqGgJe9n+NgYPJkWL3azidNsuPataYckpJsHkOlSqYkZs2yFsHIkdCnT+T5C6VK2TE5GT78cK/FmTbN9giaNMkpCIejqBDrWkxNReQTEZkhIvP9X7yFc8SRb76xPoKTTw4UxKJFdrz9dltI74UX7DotzTqkN26EM8+MizjLvBkyy5fHJXmHw7EPxNoH8RbWesgAugDvAO/GSyhHnHjwQVs7SRW++MImtnXrZqXyypWBgujTx+Y59O8PiYmmIEaMgMqVrUM6DjgF4XAUPWJVEKVVdRw2LHaRqg4BTo2fWI79RtX6GXy2bIE774ReveDyy63VcMEF0L69+U+aZP0PYK0HMFNS06bm98UX0K+fjWACNm2C117LuczSvuIUhMNR9IhVQez0ZjzPEZGrRKQvUC6Ocjn2l8ces013fPzJa1u3wquvwsCBcMklNqdBxJTAokW2F0PlykG8Vq3gu+9MwXjmpfHjbarDpZfC/ffnj7hOQTgcRY9YFcS1QBngGqA9MBAYHC+hHPnAqFGmFHbutOuZM+34zjtwww1W/RexVVUPPRR++80URIMGWecvtGplrZGUFBvZBDz/vO0Q2qmTraSxa9f+i5uXgpg500blOhyOgiNPBeFNijtTVbeo6lJVPV9V/6uqfxSAfI4ITJoEf+SW+9u3w99/27k/UmnmTBuddMYZ8OSTwdaeAKecYn0TM2cG5iUfvxXy3/9afGyA09FHw623wpo18NVX+/9MvoJYuTLyJnRPPQXnn5/VapZfzJ4dTAp3OBwBeSoIVc0ETigAWRwxcu21tulaVCZOtA19IKuCaNJkTx8CWOE+ciRot+7W0pg921oQ4Rx/vG3/edFFgBXQc+dCs2Y2AMrf4ycaP/8Mixfn/jw7dlhfRu3alr4vcjhz59pE7kh9Hqq28sdjj+V+n2g88IDpv3goH4fjQCZWE1OqiIwSkUEi0s//xVWyYoZqUKbnxbx58O+/yq7PvwpMSOH88ktwHq4gWgY7vn7/PbRpY1s5vDiji+3cBlkUxJIlsGBXHTM9deiwx23nTuu7Tkqybolvvoks+7Rpttjrvffm/jy+WclfLTySmWmet9D7ypU5/SZOtBbVzz/nfp9oLFliXSxz5uxbfIfjYCVWBVEKWAv8H9DT+50WL6GKIyNG2NYJW7bkHm77diskMzKE2f1uZcPlt+csNH/5BSpUsPNVq6yTYO5c5tU8nu3bbTpD375QpQp07gw331GCtKPOs/BhJqYLL7SadTh+IdqsmR3btzflkL1wzcy0RkdGRjA4Khq+eSmagtixw2ZaQ2QF8dFHdvRH6e4t/v396SAOh8OIdcvR8yP8Loi3cMWJH3+E9evNypMb4YXtdFpz4VvHc0jDDEaP9hxDIetwPs3T36tXw5w5ZGYq7d++isGDYfhwG8w0bJgpplKl4NHt11j4xo33pJ+WBlOm2GRqH1++pk3t2Lq1J8v0rHKOGGHdINWrWw09N/JSEAsWWAsLcioI1UBB5GXK2rLFhudmj+/ff/Lk3OM7HMWNWGdSvyUiQ7P/4i1ccSItzY5z5+YebsGC4Hxy/b58l9CdXTuV3r2VX3/F9nvesMHmLJQubS2ImTNZRAM2bkvm44/hvvvMYtShg7VaWraE5WWawogRTNT2TJtmCmT5citAf/stuOfs2WaNql3brg891EY0/fNPzudJTLT1+5YsCQr4SPgFdLt2NoDqr78sXX9R2fA8CVcQ/ny/xYvNXLZxo/0isXu3jbrq0SOr+8aNsG2bl58RFISqbV/hRlA5iiOxmpi+BL7yfuOACkAexhBHrKgGNfAspppFi0h94CtOaLKS9WtsaI+vIKqyhmHrerI1VIY3S12FqjB+zG5bcrtFC5sNXaOGtSBmzmQWtrpqyZKQng5XXBHcJiUF1qxLgDPO4LLLhSuusC2kfcJt+3PmWOvBHwlbqpRdZ29BrFhht2/QIOiEfuutrKuH+yxbZkqnalVrcbz1li3/9Oyz5u/3PyQkWLp+nnXrZqaymjXh6qv3ZFlEnnwSUlPh11+ztjR85VS1qimI7IpsxQpbFf277yKn63AczMRqYvo07PcecAYQbatRx16ycqWZlyCstjxnDrRty813leTXeTWZ9tx4wBREqRIZdGE8q7eUITER+rzSjQYsZObDn1tn9JAhVn2vXn1PC2JW5WMB28aha9esSyqlpASjg1assILU3zW0QoWsfd6zZwf9Dz6tW+dUEMuXWyvDXxV88WK46SZbvcMfgeuzbBnUqWNKp3ZtK6TLl7c9h9LTLU8qVrTuEb8F4S8Pdf31Juthh5l7JAWxeLFlyXHH2bW/IR4EfRs9elhrYn62Fcb8llF4yy0aK1aYXk5Pzzusw3EgEGsLIjtNger5KUhxxjcvlSzpKYgNG6BPH37UTozD1j5a8d542L2bBd/PpWFoAYdVsarvMcdAhcF9adGhHDNKtYPu3aF/f375Be7feM2eFsS/5dpTpYpt8TB2bDBoCaz2vGaNFczp6WZy+fpr8zvzTDP57NhhZpoFC4L+B582bayW75tqwArLWrVshCzAn3/aaq07d1qtP9wUlJoaKJ06dSwfPv7YOrg/+MDSPuQQS89XEP5K5TfcYErMH3zlK4gxY2xU77Jl9iw7d1rLpHVr67O48EKbV+G3IHr1smO4OQ0iK4jff7d0spuz3n/fTF4//URMZGRYJ/8bbuF8RxEl1j6IzSKyyf8Bo7E9Ihz5gK8gTvq/TOZM22al5axZDGn0NlWqmN/K+Vvh2GNZMG0TjSqspfUtZkz3t3du0TGFWRlNCH35NSQk8OqrcO/sswgtXwmzZjFLm9O8eeRN3lJSTAEsXx4MV/38c6hWzbad3rXLCvi0NBuddOihWeO3bm3KxZ+sDaYgwlsQvsJ54AErlD//3K5nz7bGkt83MGQIfPqpzd1r394WlP3nHyvsa9YMFMS339p969a16+rVTbEsWmQ68dxzTbGMHm0mslq1TLH162dmpqFDbRa4b77q1s38b745UBpgQ3XB8mbHDjsfPdryYurUrPngDxSIpbUBdu/Jk+G66/Ie6ZVfhELw6KOm9HPjjjusr8pRvInVxFReVSuE/Zqp6qfxFq64kJYGVSpmcPzPj7BqUxk2N27LpLf/YcLUKtxxByQnKytKNoLUVBaUaUWjAcfQ6aJmdOliLQKwjubt24Ma9IwZkKmJrE/fDdu3M2tzrRwFu09Kih3DC/gNG6xQ7tLF+hk++cQK9YQEmyAXTvaRTLt2WUukdm1TMsnJthEQ2DqB9esHCsLvkzjVW/qxffvg/JFHzAS0bJm1IHwFsXmzFfrduwcyJCSYMlq0yPpXNmywltGYMWYi69jRlOOAAdbi6NHDlOHo0RauXDmTaetW6N3bWlRgyslXqn7epqba0TfDgZkIfVNcrArCrxhs25a1TyiePPQQ3HabDV/evNnctmyx/PSn1KjaSiyPP561VZidl1+Ga66Jn6yrV9veVPu61fzdd7vW2X6jqnn+gL5AxbDrSkCfWOIW1K99+/ZaFJk9WzUUyj3MccepdkyZoR+XG6ygOnlSSAcOVC1XTnXDBtX69VXPPWWFrv/qVwXVxx/PmcYvv6iC6pdfqmZmqpYpY9czaa4bKa+g+sgjke8/cqSFffZZO4rY8dxzzf/001WrVVM99FDVE0/MGX/3btXSpVWvvdauFy+2+K+9ZteNG9t1nTp2fe21qiVLqm7erNqli2qbNtHz5q+/VNu3V/35Z9X777d0Royw4w8/ZA3btatqzZrmN2SI6kUX2X1A9bnnssq7YoW5g+phhwV+o0dbnCZN7N0lJ9v7AdVvvrEwNWrY9U03BfE++MDcSpdW7d49+vOEc999Fud//7PjvHmRw40frzptWmxpRiIzU/Xiiy1/ROwdiqgOGKD64IPB81Stqvrxx6qLFgV588EHkdP85JMgzOrV+y5bNEIh1dNOs/QPPVR10ybVYcNU16+PLf6OHfYeq1ZV3b4997D//qs6eLBqWtq+yfrEE6offrhvcYsCwESNVvZH88gSCKZEcEuNIV43YBYwF7gtgn8DbFTUNGACUDebfwVgKfBCXvcqigpi3jz7EB99VHXnTtXbb1edMydrmI0bVcuXD+llia9p6ukPKqg++aRqiRJBgXv00ar/+Y/q5Mn2xj75JOe91q7VPcpj4cLg4/2RjvoXHRRMEUTCVy6XXWbHI4+04333mf/nnwfpPf985DSOPVb1hBPs/I8/AmWlqtq5s137BeeECXb96KOqSUmqt90WW36+8YbF69RJtWJFy9NwLrjA/EuVUk1PDxQJqKam5kyvYcOscvn8+qtq5cqq9erpHsUKqi+9pLp8eZBmz56qK1eqnn++FWIpKap9+th5JHbuVP3nH9WZM63QPvNM1UaNVGfNsvRefDFnnMxMK+TatYstjyIxapSl37at6qBBqlu3ql59dfAcHTuqvvOOavPmpiz9912ihBXSPmvX2nH1alOEjRpZuPffz3q/9HTVV15RffllU7LZ8+CRR+z+Q4fa8/nMmmX5o2pxQfWMM+xYpYodb789CL9+veqPP0Z+Zv8/DfZsr75qCvGKK0w+n6FD7VnA8nj37pxpTZ2qOnCg6jnnqP75Z1a/DRusElGtWlZFFArZ//XJJyOnGYnJk+1bWLfOnuvuu1UzMmKLuz/kh4KYFsHtnzziJALzgMZAMjAVaJktzMfAYO/8/4Dh2fyfBd4/UBXEhx9aDleoYDU4yFkYXn+9qkhI/+RI3fTtr3v+1CVLBjXKPn1UW7dW/eij6IWdqtUEL7hA9euvg4/jE/rpO+WvsNbEzMjx/v3XwvoF+V13Zf3wd+xQrVTJ3JYti5zG1VdbqyUjIyhgJk0yv4ED7fqWW+w6I8M+KL8Qmjw5tvz88svguS69NKf/vfea38UX2/WaNaagK1SI/KENGGDhL7oop99nnwX3mjzZ3sfNNwd5W6OGFahPPmnXTZqoPvSQ6o03moIKhXIWDFddFaT5wgv2Tk87zcI2bpy1MPbxKwVgBdXixdb62Ru6dFGtW1d1167Abdcua52FF5YPPxzkX2KiyZuUZGH+/dfcxo0LFM6ECaZIzz/fCuspUyyd664LZE5MtIpOZqbqkiWqRxxh7mXL2vH33y3OihWmYCtUUP3pJ2s9n3SSxbvgAvu/HHKIxVe1PDvpJEtj7FhTRDfcYP/DP/4IlHq9eiYjWB4kJamefbb9H267zdy7drX3AaqPPZYz/y691OJVrGhpbNwY+L3/fvCsb75p3+g99wSVFVA95hjVp5+29+cTCqmOGWP/mRdeUN2yRbVpUwtfrVrQiv/00+jvNTPTvomPPrI82FfyQ0EMBZ4CDvF+TwFv5xHnWOC7sOvbgduzhUkD6nnnAmwK82sPfAicV5QVxKpVQc0qO3fcYR9IYmLwZ+nUKfCfOtX8Lq3/tf3zMjO1ZUurgfqFq6rV7FNSrEYvYjXASJx4ov0Z/UILVF/mUr2r/tuakJCzxu2Tnh78McEKg7ZtrTDyuf9+a4ZHY9gwi5uWZjVhCAqyO+6w6+HDg/BffWUmrYULo6eZnYkTg+fyC5ZwRo602ly4qaBzZ9X//jdyer5JbciQyP6XXmqFwo4dqs2aqfbvbyYZsJpoUpLqySebn49f0Iwfb0rlp58CvxYt7P20bq16+OGmHG+91fyuvNIUbHZzyOOPW3pJSXb/ypVNMUWqla5Yodqjh+WTz5Qpuqe1lheTJlnY5GQz+/3+u+5psb79dlDBGTLE/oebN5tMdepY67FUKVMUzZpZ4T1vXlAxeuEF1VNOMcUwcqTq/Pnm/tprVlh262bxy5dXTUiwvJg/3+Tyle1DDwX/K///VqaMtQRr1jS5k5NVW7a0fDj0UNVnnrFwAweaUrjnHt3TmgLVSy4xZRkKqfbubW433qg6d25QqWjb1t7zH3/Yc195ZZBn/fvbvdu0Mdn9/6efznvvBWbPhAQzJ44bZ//J8LB+a/Xpp82ketll9lwnnGDy/fyzmVTXrQvu/d57Qfyjj877/UYjPxREWeARYCLwN/AQUDaPOP2BN8KuB2Uv6L3WwbXeeT9AgapY5/kEoG5RURB//qm6dGlWt2XL7OX37Rs5zqmnWmFw551WMFxyiX0EfkF9882qyckhXZtU3ao/ajWJ7B+/XzPu39/+NNG4+WYrdE45JWiS38tdOrjZb1qvXvR4GRlBjaVixdzzIRppaRZ/2DB73oSE4AN79VXdUwPeH5YutXSaN4/crxMKZf2AVE2ZRrNB//13UPOLRGZmoPy7dbMPt39/q+37BaaI6uWXB3G++srce/Swo/da95gAH3ooqwJ/5x3zHz3arr//PqsM3bpZQdevn+5pWYLq66+bIvBr7arWVwRWOPucd54VotnzJdrzpqRYGoMHW74lJtr7vPFG3dPK7NUrMKO99lrWgs7vT3n2WfMPhcw8mpRk7n5fkN9Pdu21Vvv1/YYPt/OnnsopX2pqoNCrVLG+oXHjgspNWprqu+8G7+Wii+xb+uab4JvascMUSLlyZl4K/x9t3WqK33+Www83hZeQYOYeVdVrrgnyf9s2e4bLLw/kvuoq+4aXLw/SDYXsevDgIO0SJayVs2mTmW0TEuxdhfPUUxa2VasgXp069s5DIfs/Nm+uOn169P6rWNhvBbEvvxgVRG3gMyDVMyct9TrArwJu8cJEVRDAJZ7Smli/fv19z6EYqF7d7Lc+O3ZYoe/XRlTVquKrVu0JU7duSM/u8K/qunUa2rlLP/nPKwqBHbNnT9XWDTdZImPGRL23X8BWq2YFRjQWLAhaK507q1aqFNKrGo7WEw9bq8cfn/vzVa1q8Zo2zT1cNDIyrHZ49dXWvK5dO/DbvNk6O/PqrM+L3btVa9Wy2mh+EApZLWzLlrzDXn65PV+VKlb7++234KMN7xOaMSMooML/G755bMIEq1j4/n5LccsWK/wvuyxIa+dOu+eVV1rtsVIl1W+/tf4e30RTtaopH3+ggV+Y/P231bSTk7PWePPirLOyFvCtW1tF5z//0T019lq1zB6vai1Av8O7adNAEcyaFaQ5f77FO/rorKa+Dh3MvHPvvZaGb7pZtCh6BcCvjVesGPRvjBoV9O3t3h30jQwbFvkZ163LvWN9ypSg1eubofwBCjt22DcoYkoy/NP1Wzy5MXWq/QcWLMjqvmBBzorhxo1mcqtc2SoxX3xhCqJcOasMgvXV7C/50YL4HqgUdl053HwUJU6eJqZs4csBS73z94DFwEJgDbAJeCS3+8WzBbF5s+VUy5aB26efmluDBmaPVlVrh3q9iX6N8VFuti9gzBhdRi2rHT24TbVuXW1afb3+t+lU+4qj2Y00sPmC9VnkxqBBFu6yy+yDPeMMq/GefXbu8Zo1s3jHHZd3fkSjY0crvLp3t9pNPMjI2H9Fsy88/bTuab1Mnhy8XxHr6/DZti14V35tPD3dOleTkoLX7I8oCn/t4bX9HTusZhluh/YL119+sYLjggus5tmrl10fdpjJUqmStSKuvz5nYZ0Xb71lcX77za4HDTJlX7OmFcr+sz35ZBDn77+tteGbbxo3zvmO5szJartXtRp1zZr2f2ndOjb5LrjA8s0fABGJN9+0T2rRotjSjMT27fa8ycn2TOFm5O3brXXSo4f118WzI3n27Cx1Tl261MxOYJWVXIqNmMkPBZEai1s2/yRgPtAorJO6VbYwKUCCd/4gcF+EdArdxOTXChMSghfi24av6fCrJkimZuzYHYwtTUvT8ePt9FtOti/Xa7s2ZIH2bzZFd1JCE9mt/6v2auSxo2H4phCw1kRupKVZQfT666rHH28tiRIl8h4p5A/l7NMn5mzJwfXXWy24Zk1rHR1MbNliHdThHb1VqwadpuH4tVy/P+Kjj+w9HHlkEObvv4Nauo/fX3DJJUEtuEWLnAWralAAX365hatbN+gzevbZoIUSqeM7N3bvNjOZn75v5vDNJ/75+PE5486ebX6xtlgee8zCly0beaBAJFasiD5yKZxYTGp5ceGFQaWgKJGRYZWHaKMS95b8UBCTgPph1w2ByTHE6wHMxkYz/c9zuw/o5Z33B+Z4Yd4ASkZIo9AVxHffBR/GH3+Y2zXX2PDUF8verKC64rupQaC7797TObacsDZxq1Z6DsO1Jst1ZsoJCqrvMFD1gQdyvf+SJUHSsXwcCxfah96nT9AX8dJLucfp1SsonPaVRYuCkRj7k86BwvPP24it7Bx3nNW6d+2yjssePbLOE8mNLl0s/2rWtFpyXrXTtWuttZh9hNrkyTa6yB82uq/88EPw3xszxkytYMM7I/HZZ7GPsvL7a3LrBypM/GfP3jdwsJEfCqKbZ/IZDrwLLAJOiSVuQf3iqSD88fdg47tVrfBt1XyXfkpfBdXU/g+ogm5r0kavrjRM69ULabWEdA2d0DGIPGyYvtHwfgXV+8+fp6D6Fx0iD8kJY9euIIm9mZTkjyCB3JvkqlaY+J2M+8OaNWaP9m22xZGffw46m/v21T2tz2+/zTvun39aR/j+mEfyk3Xrgv/QypVmsmzVKn/SDp+vs6+T1OJJRoaZwWKplB3I5KYgbBf6PFDVb0Wkg9cpnAqMBLbHEvdgYMkSEFHKl1NSUxP2uNWvsJGa2OJAK0f+AdWr8+4xL/D8u504sfYqzgn9DznrTNi5wxbdOfVUupWcCWfBi980AqB5+/J7tvOMRokSthxGKBQsixEL1cOWU8y+1XR2/HSr7+cSjFWr2qJ1xZkTwnZwf+MNuPVWaN4cKlXKO+5RR9lChUWFypVtFd2tW2359ldesSVd8oP69W2Jk8TEnOt7FQUSE3Pfb704EJOCEJGLgGuxYadTgGOA37HJbQc9SxaFqMVKmpXZQGqq7eu8eDF0OGQZtbANClZmVIVjjuHt2cfRMmkWP8xti7ATjpts//60NKhalTpnnkCbB+Gff4RataDCxB9ikqF2bfuYIi22F41q1YJzf1XVaOSXgnBkpUoVOProwpZi/xgwINgKt3Jl++UHIrYacfnytpaWo+gRk4LAlMORwB+q2kVEDsXmQhQLFk/fSD1dzBGbpvDytBZs2SKkp0O9unOpUTMBVsIKajG7ybH8NiqJxy5JRF7fBWXL2lrYSUm2CYNH9+62CFzz5rHL8Mwztmje3uAX9hUrBltUR6NqVTuGKxWHA2yBv3jx+ed7V+lxFCyxKogdqrpDRBCRkqr6r4jsRfF2YLNkQQZtWEK77b+wg8v27EVQf90UyhzWhAqbM1i5tSbD1pxKQgIMHNIE2jxnu/Ak5czi7t3hscf2TkF06bL3cvuFfV6tB7AVWUuWzLkZkMMRT8qVK2wJHLkRq4JYKiKVsL6H70VkPdZRfdCjCkvWl6NHxU103Gh7b/r7E9df/gecfjg1FyaxsulFzJhcli5dbO8BrroqaprHH2+7m4UvVx0PfAWRV/8DmBlk61azuzocDgfEqCBUta93OkRExgMVgW/jJlURYv28dWwLVaHeMXVpsKAUDZas4uuvawBQb/c8OOxcav0NMxaVZfr0rFt5RqNECdu0Jt74JqZYWhDglIPD4cjKXncNqeqPqjpKVXfFQ6CixpLPbbeSep0awUkn0Xn3WDIyQAhRh2Vw5JHUrBlsltOxYyEKm42UFNtk58gjC1sSh8NxIOLGDuTB4inrAKjfuRGcey6dE2zbsJqspOQ7b8Chh1KzpoUtUcKGKRYVSpSw3dgGDy5sSRwOx4FIrH0QxZalS+1Yt1EJqH00nf9uDW2h/mGVYdAggD0KokMHKF26kASNghs+6HA49hVXfOTBqtU2Bs+35zduU5YGDaBJm0AT1Kplx/AJUg6Hw3Gg41oQebB6QzJVkjaRlGQTCURg3Dib3ONTr54dO3cuBAEdDocjTjgFkQfpm0tRvcxmbHts45BDsoY58UQYPRp69ChQ0RwOhyOuOBNTJJYsgQ8/BCB9ezmqld+Ra/CEBDjtNDcj1OFwHFw4BZENVeDii20BmmXLSA9VoVrljMIWy+FwOAocpyA8Ro+Gli2hdKkQr33nzSz7+WfSqebWJ3I4HMUSpyA8nr0znfXz19GARTzEHWSQSOinX1hLVarVcl01Doej+OEUBLBl3ip+nlaBgbzHI5m3sIiGfEEf1o2fSohEqtXdy2VUHQ6H4yDAKQhg/KCh7KIk3V7pQ68dH9GoETxd8jbS/10DQLVGbslJh8NR/HAKYtYsvvmzMmWTd3HCgHokJgkXXAC/7uzALGw9bqcgHA5HcaTYG9e1WXO+qdWIru2SKFnS3I44wo4/YjPfqtV0y5w6HI7iR7FvQSxcCAuXJdO9R5AVrVvbcQInAm6XNYfDUTyJq4IQkW4iMktE5orIbRH8G4jIOBGZJiITRKRumPtkEZkiImkiclm8ZGzUyFY8PeuswK1+fShXOoOptAWC/ZodDoejOBE3BSEiicCLQHegJTBARFpmC/YE8I6qHgbcBzzsua8AjlXVw4GjgdtEpHa8ZK1dGypVCpcdWjbLREmgYoltJCfH684Oh8NRdIlnC+IoYK6qzvc2F/oQ6J0tTEvgB+98vO+vqrtUdafnXjLOckak1RGmFapX2F7Qt3Y4HI4iQTwL3jrAkrDrpZ5bOFOBft55X6C8iFQFEJF6IjLNS+NRVV0eR1lz0Kq1LaxUrVnlgrytw+FwFBkKu5P6JqCziKQCnYFlQCaAqi7xTE9NgMEiUiN7ZBG5REQmisjE9PT0fBWsVSs7Vqte2FnkcDgchUM8S79lQL2w67qe2x5Udbmq9lPVI4D/eW4bsocBpgM5dntW1ddUtYOqdqiWz0ON9igIN4LJ4XAUU+KpIP4GmopIIxFJBs4CRoUHEJEUEfFluB0Y6rnXFZHS3nll4ARgVhxlzUHdurZ4X7t2BXlXh8PhKDrEbaKcqmaIyFXAd0AiMFRV00TkPmCiqo4CTgQeFhEFfgKu9KK3AJ703AV4QlX/iZeskRCBtLSCvKPD4XAULURVC1uGfKFDhw46ceLEwhbD4XA4DihEZJKqdojk53pgHQ6HwxERpyAcDofDEZGDxsQkIunAov1IIgVYk0/i5CdOrr2jqMoFRVc2J9feUVTlgn2TrYGqRhyvedAoiP1FRCZGs8MVJk6uvaOoygVFVzYn195RVOWC/JfNmZgcDofDERGnIBwOh8MREacgAl4rbAGi4OTaO4qqXFB0ZXNy7R1FVS7IZ9lcH4TD4XA4IuJaEA6Hw+GISLFXEHnteleActQTkfEiMsPbRe9az32IiCzzdtebIiI9Ckm+hSLyjyfDRM+tioh8LyJzvGOBro0uIs3D8mWKiGwSkesKI89EZKiIrBaR6WFuEfNHjOe8/9w0EYnbil9R5HpcRP717v25iFTy3BuKyPawfHslXnLlIlvUdycit3t5NktETilguUaEybRQRKZ47gWWZ7mUEfH7n6lqsf1ha0TNAxoDydj+FC0LSZZaQDvvvDwwG9tQaQhwUxHIq4VASja3x4DbvPPbsH07CvNdrgQaFEaeAZ2AdsD0vPIH6AF8g60zdgzwZwHLdTKQ5J0/GiZXw/BwhZRnEd+d9y1MxTYQa+R9t4kFJVc2/yeBuws6z3IpI+L2PyvuLYhYdr0rEFR1hapO9s43AzPJucFSUaM3MMw7Hwb0KTxR6ArMU9X9mSy5z6jqT8C6bM7R8qc3ttWuquofQCURqVVQcqnqGFXN8C7/wJbiL3Ci5Fk0egMfqupOVV0AzMW+3wKVS0QEOAP4IB73zo1cyoi4/c+Ku4KIZde7AkdEGgJHAH96Tld5TcShBW3GCUOBMSIySUQu8dxqqOoK73wlkGNTpwLkLLJ+tEUhz6LlT1H6312A1TJ9GolIqoj8KCI59mApICK9u6KSZx2BVao6J8ytwPMsWxkRt/9ZcVcQRQ4RKQd8ClynqpuAl4FDgMOBFVjztjA4QVXbAd2BK0WkU7inWpu2UIbEie030gv42HMqKnm2h8LMn2iIyP+ADOA9z2kFUF9tA68bgPdFpEIBi1Xk3l02BpC1IlLgeRahjNhDfv/PiruCyHPXu4JEREpgL/49Vf0MQFVXqWqmqoaA14lTszovVHWZd1wNfO7JscpvsnrH1YUhG6a0JqvqKk/GIpFnRM+fQv/fich5wGnAOV6hgme+WeudT8Ls/M0KUq5c3l1RyLMkoB8wwncr6DyLVEYQx/9ZcVcQee56V1B4ts03gZmq+lSYe7jNsC+2/WpBy1ZWRMr751gn53QsrwZ7wQYDXxS0bB5ZanVFIc88ouXPKOBcb5TJMcDGMBNB3BGRbsAtQC9V3RbmXk1EEr3zxkBTYH5ByeXdN9q7GwWcJSIlRaSRJ9tfBSkbcBLwr6ou9R0KMs+ilRHE839WEL3vRfmH9fTPxjT//wpRjhOwpuE0YIr36wEMB/7x3EcBtQpBtsbYCJKpQJqfT0BVYBwwBxgLVCkE2coCa4GKYW4FnmeYgloB7MZsvRdGyx9sVMmL3n/uH6BDAcs1F7NN+/+zV7yw//Xe7xRgMtCzEPIs6rvD9q2fh20/3L0g5fLc3wYuyxa2wPIslzIibv8zN5Pa4XA4HBEp7iYmh8PhcETBKQiHw+FwRMQpCIfD4XBExCkIh8PhcETEKQiHw+FwRMQpCIejCCAiJ4rIl4Uth8MRjlMQDofD4YiIUxAOx14gIgNF5C9v7f9XRSRRRLaIyNPeGv3jRKSaF/ZwEflDgn0X/HX6m4jIWBGZKiKTReQQL/lyIvKJ2F4N73kzZx2OQsMpCIcjRkSkBXAmcLyqHg5kAudgs7knqmor4EfgHi/KO8CtqnoYNpPVd38PeFFV2wLHYbN2wVbnvA5b478xcHycH8nhyJWkwhbA4TiA6Aq0B/72KvelsYXRQgQLuL0LfCYiFYFKqvqj5z4M+Nhb06qOqn4OoKo7ALz0/lJvnR+xHcsaAr/E/akcjig4BeFwxI4Aw1T19iyOIndlC7ev69fsDDvPxH2fjkLGmZgcjtgZB/QXkeqwZy/gBth31N8Lczbwi6puBNaHbSAzCPhRbSewpSLSx0ujpIiUKciHcDhixdVQHI4YUdUZInIntrNeArba55XAVuAoz2811k8BtvTyK54CmA+c77kPAl4Vkfu8NE4vwMdwOGLGrebqcOwnIrJFVcsVthwOR37jTEwOh8PhiIhrQTgcDocjIq4F4XA4HI6IOAXhcDgcjog4BeFwOByOiDgF4XA4HI6IOAXhcDgcjog4BeFwOByOiPw/Zfsb4wDQE6wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "# plt.plot(range(epoch+1), history['train_loss'], label='Loss', color='red')\n",
    "# plt.plot(range(epoch+1), history['val_loss'], label='Loss', color='blue')\n",
    "plt.plot(range(200), history['train_loss'][0:200], label='Loss', color='red')\n",
    "plt.plot(range(200), history['val_loss'][0:200], label='Loss', color='blue')\n",
    "\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(range(200), history['train_acc'][0:200], label='Accuracy', color='red')\n",
    "plt.plot(range(200), history['val_acc'][0:200], label='Accuracy', color='blue')\n",
    "\n",
    "plt.title('Accuracy history')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b9d795c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total CAM: 2739\n",
      "number of velocity CAM: 257\n",
      "percentage of velocity CAM: 9.382986491420226 %\n",
      "\n",
      "accuracy for all CAM: 94.88864549105513\n",
      "accuracy for velocity CAM: 65.75875486381322 \n",
      "\n",
      "accuracy of all CAM if plus minus 100ms for velocity CAM: 97.2252646951442\n",
      "accuracy increased: 2.336619204089075 %p\n",
      "\n",
      "number of velocity CAM correct: 169\n",
      "number of velocity CAM wrong: 88\n",
      "number of velocity CAM correct increased: 64 \n",
      "\n",
      "accuracy of velocity CAM if plus minus 100ms:  90.6614785992218\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\widen\\\\Desktop\\\\predict_CAM\\\\MLP\\\\final\")\n",
    "\n",
    "\n",
    "# f = open('2_raw_base_labelled_all_set_predict_results.csv', 'r', encoding='utf-8')\n",
    "f = open('final_2_raw_base_labelled_test_set_predict_results.csv', 'r', encoding='utf-8')\n",
    "filereader = csv.reader(f)\n",
    "\n",
    "\n",
    "\n",
    "temp_list = []\n",
    "\n",
    "for line in filereader:\n",
    "        temp_list.append(line)\n",
    "\n",
    "f.close()\n",
    "\n",
    "total = 0\n",
    "vel_CAM_num = 0\n",
    "acc = 0\n",
    "acc_pm100ms = 0\n",
    "velocity_wrong = 0\n",
    "vel_CAM_correct = 0\n",
    "\n",
    "for line in temp_list:\n",
    "        if line[4] == '1' or line[6] == '1':\n",
    "                vel_CAM_num += 1\n",
    "                total += 1\n",
    "                if line[9] == line[15]:\n",
    "                        acc += 1\n",
    "                        acc_pm100ms += 1\n",
    "                        vel_CAM_correct += 1\n",
    "\n",
    "                else:\n",
    "                        velocity_wrong += 1\n",
    "                        if (int(line[15]) + 1 == int(line[9])) or (int(line[15]) - 1 == int(line[9])):\n",
    "                                acc_pm100ms += 1\n",
    "\n",
    "\n",
    "        else:\n",
    "                total += 1\n",
    "                if line[9] == line[15]:\n",
    "                        acc += 1\n",
    "                        acc_pm100ms += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"number of total CAM:\", total)\n",
    "print(\"number of velocity CAM:\", vel_CAM_num)\n",
    "print(\"percentage of velocity CAM:\",  (vel_CAM_num/total)*100, \"%\\n\")\n",
    "\n",
    "print(\"accuracy for all CAM:\", (acc/total)*100)\n",
    "print(\"accuracy for velocity CAM:\", (vel_CAM_correct/vel_CAM_num)*100, \"\\n\")\n",
    "\n",
    "print(\"accuracy of all CAM if plus minus 100ms for velocity CAM:\", (acc_pm100ms/total)*100)\n",
    "print(\"accuracy increased:\", (acc_pm100ms/total)*100-(acc/total)*100, \"%p\\n\")\n",
    "\n",
    "print(\"number of velocity CAM correct:\", vel_CAM_correct)\n",
    "print(\"number of velocity CAM wrong:\", velocity_wrong)\n",
    "print(\"number of velocity CAM correct increased:\", acc_pm100ms-acc, \"\\n\")\n",
    "\n",
    "print(\"accuracy of velocity CAM if plus minus 100ms: \", 100*(acc_pm100ms-acc+vel_CAM_correct)/vel_CAM_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ab2025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\widen\\\\Desktop\\\\predict_CAM\\\\MLP\\\\final\")\n",
    "finalset = TrainDataset('2_raw_base_labelled_test_set.csv')\n",
    "finalLoader = torch.utils.data.DataLoader(finalset, batch_size=1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc3a9513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.6150298119 sec\n",
      "2739\n",
      "Average Computation Time:  0.0038755128922450277\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print_pred(model, finalLoader, device)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start:.10f} sec\")\n",
    "print(len(finalLoader))\n",
    "print(\"Average Computation Time: \", (end-start)/len(finalLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c934805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('loss.csv','w', newline='')\n",
    "wr = csv.writer(f)\n",
    "\n",
    "wr.writerow(['train loss', 'validation loss'])\n",
    "\n",
    "for i in range(200):\n",
    "    wr.writerow([history['train_loss'][i], history['val_loss'][i]])\n",
    "    \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
